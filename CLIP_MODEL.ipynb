{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5173923,"sourceType":"datasetVersion","datasetId":3007402},{"sourceId":9192360,"sourceType":"datasetVersion","datasetId":5557058},{"sourceId":9205070,"sourceType":"datasetVersion","datasetId":5565663},{"sourceId":9209952,"sourceType":"datasetVersion","datasetId":5568917},{"sourceId":9211475,"sourceType":"datasetVersion","datasetId":5569939}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ftfy","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:53:46.407988Z","iopub.execute_input":"2024-08-22T09:53:46.408800Z","iopub.status.idle":"2024-08-22T09:53:58.548995Z","shell.execute_reply.started":"2024-08-22T09:53:46.408764Z","shell.execute_reply":"2024-08-22T09:53:58.547806Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (6.2.3)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nfrom PIL import Image\nimport os\nimport hashlib\nimport urllib\nimport warnings\nfrom packaging import version\nfrom typing import Union, List , Tuple\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nimport gzip\nimport html\nfrom functools import lru_cache\nimport ftfy\nimport regex as re\nfrom IPython.display import display\n\n\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC\n\n\nif version.parse(torch.__version__) < version.parse(\"1.7.1\"):\n    warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:58:01.961316Z","iopub.execute_input":"2024-08-22T09:58:01.962211Z","iopub.status.idle":"2024-08-22T09:58:01.970673Z","shell.execute_reply.started":"2024-08-22T09:58:01.962177Z","shell.execute_reply":"2024-08-22T09:58:01.969639Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# **CLIP Model Methods:**","metadata":{}},{"cell_type":"markdown","source":"**MODEL CODE:**","metadata":{}},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.relu2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x[:1], key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n        return x.squeeze(0)\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.input_resolution = input_resolution\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        def stem(x):\n            x = self.relu1(self.bn1(self.conv1(x)))\n            x = self.relu2(self.bn2(self.conv2(x)))\n            x = self.relu3(self.bn3(self.conv3(x)))\n            x = self.avgpool(x)\n            return x\n\n        x = x.type(self.conv1.weight.dtype)\n        x = stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)\n\n        self.transformer = Transformer(width, layers, heads)\n\n        self.ln_post = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, 0, :])\n\n        if self.proj is not None:\n            x = x @ self.proj\n\n        return x\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,\n                 vocab_size: int,\n                 transformer_width: int,\n                 transformer_heads: int,\n                 transformer_layers: int\n                 ):\n        super().__init__()\n\n        self.context_length = context_length\n\n        if isinstance(vision_layers, (tuple, list)):\n            vision_heads = vision_width * 32 // 64\n            self.visual = ModifiedResNet(\n                layers=vision_layers,\n                output_dim=embed_dim,\n                heads=vision_heads,\n                input_resolution=image_resolution,\n                width=vision_width\n            )\n        else:\n            vision_heads = vision_width // 64\n            self.visual = VisionTransformer(\n                input_resolution=image_resolution,\n                patch_size=vision_patch_size,\n                width=vision_width,\n                layers=vision_layers,\n                heads=vision_heads,\n                output_dim=embed_dim\n            )\n\n        self.transformer = Transformer(\n            width=transformer_width,\n            layers=transformer_layers,\n            heads=transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = vocab_size\n        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n        self.ln_final = LayerNorm(transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        if isinstance(self.visual, ModifiedResNet):\n            if self.visual.attnpool is not None:\n                std = self.visual.attnpool.c_proj.in_features ** -0.5\n                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n\n            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n                for name, param in resnet_block.named_parameters():\n                    if name.endswith(\"bn3.weight\"):\n                        nn.init.zeros_(param)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    @property\n    def dtype(self):\n        return self.visual.conv1.weight.dtype\n\n    def encode_image(self, image):\n        return self.visual(image.type(self.dtype))\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x).type(self.dtype)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # normalized features\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n\n        # cosine similarity as logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n\n        # shape = [global_batch_size, global_batch_size]\n        return logits_per_image, logits_per_text\n\n\ndef convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n\n\ndef build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_resolution = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n\n    model = CLIP(\n        embed_dim,\n        image_resolution, vision_layers, vision_width, vision_patch_size,\n        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        if key in state_dict:\n            del state_dict[key]\n\n    convert_weights(model)\n    model.load_state_dict(state_dict)\n    return model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:53:58.562299Z","iopub.execute_input":"2024-08-22T09:53:58.562664Z","iopub.status.idle":"2024-08-22T09:53:58.644406Z","shell.execute_reply.started":"2024-08-22T09:53:58.562632Z","shell.execute_reply":"2024-08-22T09:53:58.643600Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**TOKENIZER CODE:**","metadata":{}},{"cell_type":"code","source":"File_directory=\"/kaggle/input/bpe-simple-vocab-16e6-txt-zip\"\n@lru_cache()\ndef default_bpe():\n    return os.path.join(File_directory,\"bpe_simple_vocab_16e6.txt\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n          # Open and read the BPE file correctly\n        with open(bpe_path, \"r\", encoding=\"utf-8\") as file:\n            merges = file.read().split('\\n')\n#          merges = (bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:53:58.647296Z","iopub.execute_input":"2024-08-22T09:53:58.647630Z","iopub.status.idle":"2024-08-22T09:53:58.673165Z","shell.execute_reply.started":"2024-08-22T09:53:58.647605Z","shell.execute_reply":"2024-08-22T09:53:58.672282Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**CLIP MODEL METHODS:**","metadata":{}},{"cell_type":"code","source":"__all__ = [\"available_models\", \"load\", \"tokenize\"]\n_tokenizer = SimpleTokenizer()\n\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n}\n\n\ndef _download(url: str, root: str):\n    os.makedirs(root, exist_ok=True)\n    filename = os.path.basename(url)\n\n    expected_sha256 = url.split(\"/\")[-2]\n    download_target = os.path.join(root, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n            return download_target\n        else:\n            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n        raise RuntimeError(\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef _convert_image_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\ndef _transform(n_px):\n    return Compose([\n        Resize(n_px, interpolation=BICUBIC),\n        CenterCrop(n_px),\n        _convert_image_to_rgb,\n        ToTensor(),\n        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n    ])\n\n\ndef available_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list(_MODELS.keys())\n\n\ndef load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n\n    device : Union[str, torch.device]\n        The device to put the loaded model\n\n    jit : bool\n        Whether to load the optimized JIT model or more hackable non-JIT model (default).\n\n    download_root: str\n        path to download the model files; by default, it uses \"~/.cache/clip\"\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if name in _MODELS:\n        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n\n    with open(model_path, 'rb') as opened_file:\n        try:\n            # loading JIT archive\n            model = torch.jit.load(opened_file, map_location=device if jit else \"cpu\").eval()\n            state_dict = None\n        except RuntimeError:\n            # loading saved state dict\n            if jit:\n                warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n                jit = False\n            state_dict = torch.load(opened_file, map_location=\"cpu\")\n\n    if not jit:\n        model = build_model(state_dict or model.state_dict()).to(device)\n        if str(device) == \"cpu\":\n            model.float()\n        return model, _transform(model.visual.input_resolution)\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def _node_get(node: torch._C.Node, key: str):\n        \"\"\"Gets attributes of a node which is polymorphic over return type.\n        \n        From https://github.com/pytorch/pytorch/pull/82628\n        \"\"\"\n        sel = node.kindOf(key)\n        return getattr(node, sel)(key)\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(_node_get(node, \"value\")).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 on CPU\n    if str(device) == \"cpu\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if _node_get(inputs[i].node(), \"value\") == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n\n        model.float()\n\n    return model, _transform(model.input_resolution.item())\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    truncate: bool\n        Whether to truncate the text in case its encoding is longer than the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    if version.parse(torch.__version__) < version.parse(\"1.8.0\"):\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n    else:\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            if truncate:\n                tokens = tokens[:context_length]\n                tokens[-1] = eot_token\n            else:\n                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n        result[i, :len(tokens)] = torch.tensor(tokens)\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:53:58.674697Z","iopub.execute_input":"2024-08-22T09:53:58.675290Z","iopub.status.idle":"2024-08-22T09:53:58.822729Z","shell.execute_reply.started":"2024-08-22T09:53:58.675249Z","shell.execute_reply":"2024-08-22T09:53:58.821880Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# **CLIP EMBEDDINGS:**","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = load(\"ViT-B/32\", device=device)\n\n\n# Load COCO dataset\ndata_dir = '/kaggle/input'  # Update this to your COCO dataset path\ndata_dir_2='/kaggle/input/whuewi'\nannotation_file = os.path.join(data_dir_2, 'caption.json')\n\nwith open(annotation_file, 'r') as f:\n    coco_annotations = json.load(f)\n\n# Directory containing images\nimage_dir = '/kaggle/input/val-dummy/'  # Update this to your images directory\n\ntext_embeddings=[]\nimage_embeddings=[]\n\n\n# Process each image and its captions\nfor image_info in coco_annotations['images']:\n    image_id = image_info['id']\n    image_file = image_info['file_name']\n    image_path = os.path.join(image_dir, image_file)\n\n    # Preprocess the image\n    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n \n    \n    # Get captions for this image\n    captions = [ann['caption'] for ann in coco_annotations['annotations'] if ann['image_id'] == image_id]\n    \n    # Tokenize captions\n    text = tokenize(captions).to(device)\n    \n    # Compute image and text features\n    with torch.no_grad():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n        \n    image_embeddings.append(image_features)\n    text_embeddings.append(text_features)\n#     print(image_id)\n#     print(\"hi\")\n#     print(text.shape)\n        \n#         # Compute similarity\n#         logits_per_image, logits_per_text = model(image, text)\n#         probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n    \n#     # Print results for this image\n#     print(f\"Image ID: {image_id}, Label probs: {probs}\")\n    # Print the image and text embeddings\n#     print(f\"Image ID: {image_id}\")\n#     print(f\"Image Embeddings: {image_features}\")\n#     print(f\"Text Embeddings: {text_features}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-22T09:53:58.823928Z","iopub.execute_input":"2024-08-22T09:53:58.824249Z","iopub.status.idle":"2024-08-22T09:54:03.567534Z","shell.execute_reply.started":"2024-08-22T09:53:58.824209Z","shell.execute_reply":"2024-08-22T09:54:03.566662Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(text_embeddings[0].shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:54:03.568813Z","iopub.execute_input":"2024-08-22T09:54:03.569122Z","iopub.status.idle":"2024-08-22T09:54:03.573769Z","shell.execute_reply.started":"2024-08-22T09:54:03.569097Z","shell.execute_reply":"2024-08-22T09:54:03.572889Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"torch.Size([2, 512])\n","output_type":"stream"}]},{"cell_type":"code","source":"class CLIPDataset(Dataset):\n    def __init__(self, image_embeddings, text_embeddings):\n        self.image_embeddings = image_embeddings\n        self.text_embeddings = text_embeddings\n\n    def __len__(self):\n        return len(self.image_embeddings)\n\n    def __getitem__(self, idx):\n        image_embedding = self.image_embeddings[idx]\n        text_embedding = self.text_embeddings[idx]\n        \n        # Ensure they are tensors\n        if not isinstance(image_embedding, torch.Tensor):\n            image_embedding = torch.tensor(image_embedding, dtype=torch.float32)\n        \n        if not isinstance(text_embedding, torch.Tensor):\n            text_embedding = torch.tensor(text_embedding, dtype=torch.float32)\n        \n        return text_embedding, image_embedding\n\n# Create dataset\ndataset = CLIPDataset(image_embeddings, text_embeddings)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:54:03.574937Z","iopub.execute_input":"2024-08-22T09:54:03.575190Z","iopub.status.idle":"2024-08-22T09:54:03.588681Z","shell.execute_reply.started":"2024-08-22T09:54:03.575169Z","shell.execute_reply":"2024-08-22T09:54:03.587854Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Example Prior Model\nclass PriorModel(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super(PriorModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 512)\n        self.fc2 = nn.Linear(512, latent_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize Prior Model\ninput_dim = 512  # Dimension of CLIP text embedding\nlatent_dim = 512  # Dimension of latent representation (same as CLIP image embedding)\nprior_model = PriorModel(input_dim, latent_dim).to(device)\n\n# Example training loop\noptimizer = optim.Adam(prior_model.parameters(), lr=1e-4)\ncriterion = nn.MSELoss()\n\n# Assuming you have text_embedding (from CLIP) and image_embedding (from CLIP) pairs\nnum_epochs = 10  # Number of epochs for training\n\nfor epoch in range(num_epochs):\n    \n    for text_embedding, image_embedding in dataloader:\n        text_embedding = text_embedding.to(device)\n        image_embedding = image_embedding.to(device)\n\n        optimizer.zero_grad()\n        # Ensure the text embedding has the same dtype as the model weights\n        text_embedding = text_embedding.to(dtype=prior_model.fc1.weight.dtype)\n        image_embedding = image_embedding.to(dtype=prior_model.fc1.weight.dtype)\n        # Forward pass through prior model\n        latent = prior_model(text_embedding)\n        image_embedding = image_embedding.squeeze(1)\n        \n#  # Ensure latent and image_embedding have the same shape\n#         if latent.shape != image_embedding.shape:\n#             image_embedding = image_embedding.squeeze(1)  # Adjust as needed to match shapes\n#             latent = latent.squeeze(1) \n\n#         # Adjust shapes if they do not match\n#         if latent.shape != image_embedding.shape:\n#             if latent.shape[1] == 1:\n#                 latent = latent.squeeze(1)  # Remove the dimension if its size is 1\n#             if image_embedding.shape[1] == 1:\n#                 image_embedding = image_embedding.squeeze(1)\n#         latent = latent.squeeze(1)  # This will remove the second dimension\n\n        latent = latent[:, 0, :]\n#         print(latent.shape)\n#         print(image_embedding.shape)\n        \n       \n\n        # Compute loss with respect to actual CLIP image embedding\n        loss = criterion(latent, image_embedding)\n\n        # Backpropagation and optimization\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n    \n# Save the trained model\ntorch.save(prior_model.state_dict(), 'prior_model.pth')\n\n# Inference: Load the trained model and get latent vectors\ndef get_latent_vector(text_embedding):\n    prior_model.eval()  # Set the model to evaluation mode\n    text_embedding = text_embedding.to(device)\n    text_embedding = text_embedding.to(dtype=prior_model.fc1.weight.dtype)\n    with torch.no_grad():\n        latent = prior_model(text_embedding)\n    return latent\n\n# Example inference\n# Assuming you have a new text_embedding tensor for inference\nnew_text_embedding = torch.randn(1, input_dim).to(device)  # Example tensor\nlatent_vector = get_latent_vector(new_text_embedding)\nprint(f'Latent vector : {latent_vector}')","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:54:03.590269Z","iopub.execute_input":"2024-08-22T09:54:03.590579Z","iopub.status.idle":"2024-08-22T09:54:03.665360Z","shell.execute_reply.started":"2024-08-22T09:54:03.590551Z","shell.execute_reply":"2024-08-22T09:54:03.664494Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.2160\nEpoch [2/10], Loss: 0.2100\nEpoch [3/10], Loss: 0.2041\nEpoch [4/10], Loss: 0.1983\nEpoch [5/10], Loss: 0.1926\nEpoch [6/10], Loss: 0.1967\nEpoch [7/10], Loss: 0.1811\nEpoch [8/10], Loss: 0.1863\nEpoch [9/10], Loss: 0.1813\nEpoch [10/10], Loss: 0.1762\nLatent vector : tensor([[ 4.1519e-02,  9.8115e-02,  9.2854e-03, -1.4828e-01, -2.2102e-01,\n         -4.2182e-02, -1.8559e-01,  9.9882e-03,  2.1867e-01, -7.0503e-03,\n          9.8619e-02, -9.7428e-02, -1.4173e-01,  2.2158e-01,  4.8637e-01,\n          2.1856e-01,  1.3625e-01,  2.2066e-01, -4.8097e-03, -1.4162e-01,\n         -1.8977e-01, -1.7647e-01,  1.8642e-01, -2.0264e-01,  2.1682e-02,\n          1.6954e-01,  3.4865e-01,  1.0168e-01,  1.1873e-01, -2.1007e-01,\n         -3.0366e-01,  2.1498e-01, -1.1840e-01, -2.6465e-01,  8.8176e-02,\n         -1.4131e-01, -2.9785e-02,  1.9188e-01, -2.9576e-01, -2.3683e-01,\n          2.4765e-01,  6.2563e-03,  7.2375e-02,  1.5617e-01,  1.7519e-01,\n         -2.3430e-01,  1.1340e-01,  3.5043e-01, -5.5040e-01, -2.8679e-01,\n          1.6078e-01,  7.7398e-02, -8.6085e-02,  2.5977e-01, -4.4268e-01,\n          1.7808e-01,  3.3399e-01,  4.2773e-01, -3.1104e-01,  1.9543e-02,\n          2.7051e-01, -3.0807e-01,  7.4557e-03,  7.0886e-02,  1.9426e-02,\n         -3.3964e-01, -2.8908e-01,  3.2073e-01,  1.8103e-01,  4.1606e-01,\n         -1.2458e-01, -6.8027e-02,  2.3017e-01, -1.0884e-01, -1.3397e-01,\n         -1.3669e-01,  4.0767e-02,  8.7885e-02, -2.4436e-01,  7.8142e-03,\n         -2.4640e-02,  3.6526e-01, -5.0846e-02,  7.6914e-02, -2.5340e-01,\n         -2.8143e-01, -4.6961e-02,  1.2926e-02, -1.8800e-01,  1.2703e-01,\n          7.5844e-02, -1.7191e-01, -1.5242e-02,  3.5610e-01,  2.6424e-01,\n          3.1168e-01,  1.1275e-01,  4.4061e-01, -2.0773e-02, -1.9922e-01,\n          1.6104e-01,  1.2201e-01,  2.5366e-01, -3.6506e-01,  4.9078e-03,\n          5.0908e-02,  5.8374e-02, -5.6701e-05,  3.8352e-01,  1.3833e-01,\n         -1.3200e-01, -3.3844e-02,  2.3928e-02,  4.1024e-01, -1.4954e-01,\n          2.1620e-01,  2.7453e-01,  1.6670e-01,  5.9989e-02, -1.1126e-01,\n          1.7544e-01,  2.3103e-02,  4.4253e-01,  1.3285e-01, -1.6657e-01,\n         -2.0588e-01, -1.7119e-01,  2.8081e-01,  6.0007e-02,  2.3014e-01,\n         -1.3173e-02,  1.4613e-02,  3.0798e-02,  1.5109e-01,  4.7682e-01,\n         -4.1285e-01,  2.2196e-01, -2.7078e-01, -1.7901e-01, -2.8386e-01,\n          3.2925e-01,  4.2706e-02, -2.8740e-02,  1.6921e-01,  3.1703e-01,\n         -3.2996e-01,  5.3248e-02, -1.2182e-01, -9.5218e-02,  1.3222e-01,\n          2.1900e-01,  1.8921e-01, -9.6786e-02, -1.6690e-01, -1.3248e-01,\n         -1.7693e-01,  2.7450e-03, -8.0209e-03,  3.6633e-02, -1.7646e-01,\n         -2.8061e-01, -5.2500e-01, -1.1018e-01,  7.4356e-02, -1.1777e-01,\n         -2.8538e-01,  1.9452e-01,  2.1220e-01, -1.7709e-01, -3.3686e-01,\n         -1.8067e-01,  1.8886e-01, -3.3503e-01,  2.2935e-01,  2.8240e-01,\n         -4.6529e-01, -2.6947e-01, -3.4165e-01,  2.8471e-01,  2.2817e-01,\n          1.7032e-01, -3.1702e-01,  5.3722e-01, -3.1684e-02, -1.5813e-01,\n          2.8572e-01, -2.2441e-01, -1.6613e-01, -3.2660e-02,  1.3248e-01,\n          3.4573e-01,  2.6755e-01,  1.4806e-01,  1.9492e-01, -2.0987e-01,\n          1.6438e-01, -2.2550e-01,  1.0313e-01, -3.2515e-01,  8.4321e-02,\n         -7.2057e-03, -3.7685e-01, -1.4636e-01,  1.5269e-01,  1.2381e-01,\n          2.0501e-01, -7.0208e-02,  2.0878e-01,  4.8826e-02, -1.5156e-01,\n          1.2410e-01,  2.5508e-01, -1.3054e-01, -5.7563e-01, -4.7245e-01,\n          2.7993e-01,  1.4219e-01,  2.8707e-02, -1.6004e-02,  4.2884e-02,\n         -1.4539e-01,  1.2581e-01,  1.5252e-01,  3.5015e-01,  6.8186e-02,\n          1.6387e-01, -2.0184e-01,  8.8760e-02,  2.5235e-01, -4.6376e-01,\n         -4.1351e-01, -1.6677e-01,  3.8306e-01,  2.3229e-02,  2.0621e-01,\n         -5.8924e-02, -2.5152e-01, -7.2760e-02, -1.4771e-01, -2.9064e-01,\n          1.1847e-01,  3.4168e-01, -8.6287e-02, -2.6407e-03, -1.3239e-01,\n          3.1553e-02,  1.4583e-01, -4.3721e-03,  2.3522e-01,  2.2361e-02,\n          6.8268e-02,  7.7503e-02, -8.3660e-02,  8.4721e-02,  1.7677e-01,\n          5.3240e-01,  4.4036e-01,  3.3445e-01,  3.2135e-02,  2.3817e-01,\n          5.9361e-02,  9.9639e-02,  2.5803e-01, -2.4878e-01, -3.6708e-01,\n         -2.9429e-02, -2.6630e-01,  2.9958e-01,  1.3118e-02, -3.2552e-01,\n          2.4402e-01,  1.6572e-01,  8.8039e-02,  5.6696e-01, -3.1749e-01,\n         -1.5463e-02, -2.3517e-02,  4.0016e-02,  1.7902e-01,  3.0708e-01,\n          2.2005e-01,  1.3807e-01, -3.3786e-01, -6.3796e-01, -1.7297e-01,\n         -4.1785e-01, -6.9145e-02, -3.7440e-01, -6.3816e-02, -2.0833e-01,\n         -2.1866e-01, -1.1491e-01, -2.2313e-01, -8.6125e-02, -2.1269e-02,\n          3.6108e-01,  1.1734e-01,  4.9212e-01, -3.3311e-02, -2.5113e-02,\n         -1.1984e-01, -2.0617e-01, -3.3239e-01,  4.8749e-01,  2.2887e-01,\n          1.0174e-01, -4.2797e-02,  1.1421e-01, -5.7697e-01, -2.1602e-01,\n         -8.0811e-02, -2.0075e-01, -2.7617e-01,  8.0650e-03,  2.7206e-01,\n         -1.0686e-01,  1.6409e-01, -4.2839e-02, -1.2380e-01,  3.1552e-01,\n          2.3781e-01, -2.2186e-01, -5.5005e-02, -7.3206e-02,  3.5589e-01,\n         -1.2073e-01,  3.1473e-01,  2.0268e-01,  2.5851e-02, -4.4074e-01,\n         -1.2606e-01,  3.0336e-01,  1.7637e-01,  4.0004e-01, -4.5383e-01,\n          2.7233e-01,  2.8765e-01,  4.9595e-01,  2.6520e-01, -1.5128e-01,\n         -1.8081e-01, -5.0154e-01,  8.7229e-03,  1.9002e-01, -3.7052e-01,\n          4.4866e-01, -5.9659e-02, -1.5225e-01,  1.3833e-01,  5.3622e-02,\n         -1.7239e-01, -2.4844e-01, -5.1637e-02,  1.9780e-01, -4.1131e-02,\n         -3.2670e-01, -1.3984e-01,  1.7492e-01,  6.0405e-01,  3.1045e-01,\n          5.5666e-01, -9.8893e-02,  2.1649e-01, -1.9279e-01, -4.6549e-02,\n          2.2405e-01, -2.6750e-01, -3.0455e-01,  8.8474e-02, -3.5945e-02,\n          9.7683e-02,  3.7798e-01, -2.4648e-01,  1.2878e-02, -2.5684e-01,\n         -3.0108e-01,  7.2033e-01, -1.2569e-01,  6.8403e-01,  1.8049e-01,\n         -3.1375e-01, -5.5683e-01,  1.2698e-01,  3.2946e-01,  1.7102e-01,\n         -2.7040e-01,  2.0087e-01, -1.8886e-01,  3.1981e-01,  3.2271e-02,\n         -2.9056e-01,  3.1332e-01, -1.7760e-01, -2.2702e-02,  5.6583e-01,\n         -3.8311e-01,  4.6717e-02,  2.7476e-01, -7.9980e-02,  1.0063e-01,\n          2.0558e-01, -1.4939e-01, -3.3384e-01, -8.4564e-02, -6.4599e-02,\n         -3.6068e-01,  2.1154e-01,  2.1294e-01, -3.4393e-01, -5.9344e-02,\n          2.1490e-01, -1.4479e-01,  2.5335e-01,  1.4006e-01, -2.7812e-01,\n         -3.7189e-01,  9.1436e-02, -6.6109e-03,  3.7730e-01, -2.0570e-01,\n         -1.3689e-01,  1.0559e-01,  4.2481e-01,  1.0510e-01, -2.8447e-01,\n         -1.1117e-01, -1.6320e-01,  3.6743e-01,  3.0980e-01, -1.5898e-01,\n         -1.3639e-01,  2.1276e-01,  8.2481e-02,  4.2357e-01, -2.5661e-01,\n          1.9001e-01,  4.4177e-02,  1.6614e-01, -1.3741e-01, -3.5791e-02,\n         -3.8081e-01, -1.6420e-01,  1.7961e-02, -2.4402e-01,  8.2798e-02,\n          3.0680e-01,  1.7502e-01, -3.1895e-01,  1.0810e-02,  3.2845e-01,\n         -4.3721e-02, -3.2188e-01, -8.9745e-02,  2.1092e-01,  2.2502e-01,\n         -1.6448e-02, -2.6687e-02,  1.2104e-01,  2.5442e-02,  2.2494e-02,\n          3.4727e-02,  3.6531e-01,  1.3695e-01,  6.4927e-02,  3.8477e-03,\n         -2.7553e-01, -2.2272e-02, -3.1312e-01,  2.3400e-01, -8.9415e-02,\n          1.3444e-01, -5.4842e-02,  1.6059e-01,  1.6455e-01,  7.4851e-02,\n          1.0884e-01, -3.3908e-01,  2.7011e-01, -1.8686e-01, -1.1189e-01,\n         -3.1879e-01, -1.0628e-01,  3.3056e-01, -1.8972e-01,  1.1817e-01,\n         -5.6043e-02,  1.6971e-01,  4.4336e-01, -8.0194e-02, -1.5803e-02,\n         -3.2325e-01,  2.4902e-01, -1.1506e-01, -3.1057e-01,  4.1798e-02,\n         -1.6949e-01, -1.4984e-02,  1.4278e-01,  2.8747e-01, -3.7198e-01,\n         -5.8456e-02,  5.0291e-01,  1.9057e-02, -1.2930e-01,  1.4197e-01,\n          2.5867e-01,  6.9086e-02, -4.3036e-01, -3.1003e-01, -1.1108e-01,\n          2.5180e-02,  1.8107e-02]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class DiffusionDecoder(nn.Module):\n#     def __init__(self, latent_dim, text_embedding_dim, image_embedding_dim, hidden_dim):\n#         super(DiffusionDecoder, self).__init__()\n#         self.latent_dim = latent_dim\n#         self.text_embedding_dim = text_embedding_dim\n#         self.image_embedding_dim = image_embedding_dim\n#         self.hidden_dim = hidden_dim\n        \n#         # Combine text and latent embeddings\n#         self.fc1 = nn.Linear(latent_dim + text_embedding_dim, hidden_dim)\n#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n#         self.fc3 = nn.Linear(hidden_dim, image_embedding_dim)\n#         self.relu = nn.ReLU()\n\n#     def forward(self, latent, text_embedding):\n#         # Concatenate latent vector and text embedding\n#         combined_input = torch.cat([latent, text_embedding], dim=-1)\n        \n#         # Pass through fully connected layers\n#         x = self.relu(self.fc1(combined_input))\n#         x = self.relu(self.fc2(x))\n#         output = self.fc3(x)\n        \n#         return output\n\n# # Initialize Diffusion Decoder\n# latent_dim = 512  # Dimension of latent vector from prior model\n# text_embedding_dim = 512  # Dimension of text embedding from CLIP\n# image_embedding_dim = 512  # Dimension of CLIP image embedding\n# hidden_dim = 1024  # Hidden dimension size\n\n# diffusion_decoder = DiffusionDecoder(latent_dim, text_embedding_dim, image_embedding_dim, hidden_dim).to(device)\n\n# # Example usage\n# def generate_image_from_latent(latent, text_embedding):\n#     latent = latent.to(device)\n#     text_embedding = text_embedding.to(device)\n#     with torch.no_grad():\n#         generated_image_embedding = diffusion_decoder(latent, text_embedding)\n#     return generated_image_embedding\n\n# # Example latent and text embeddings\n# example_latent = torch.randn(1, latent_dim).to(device)  # Example latent vector\n# example_text_embedding = torch.randn(1, text_embedding_dim).to(device)  # Example text embedding\n\n# # Generate image embedding\n# generated_image_embedding = generate_image_from_latent(example_latent, example_text_embedding)\n# print(f'Generated image embedding shape: {generated_image_embedding}')\n","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom PIL import Image\n\n\n# Define Diffusion Decoder Model\nclass DiffusionDecoder(nn.Module):\n    def __init__(self, latent_dim, image_size):\n        super(DiffusionDecoder, self).__init__()\n        self.fc1 = nn.Linear(latent_dim, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, image_size * image_size * 3)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = torch.sigmoid(x)\n        x = x.view(-1, 3, image_size, image_size)\n        return x\n\n# Initialize Models\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ninput_dim = 512\nlatent_dim = 512\nimage_size = 64\n\n\ndiffusion_decoder = DiffusionDecoder(latent_dim, image_size).to(device)\n\n# Generate Latent Vector\ndef generate_latent(text_embedding):\n    text_embedding = text_embedding.to(device)\n    with torch.no_grad():\n        latent = prior_model(text_embedding)\n        latent = latent.squeeze(1)\n    return latent\n\n# Generate Image from Latent Vector\ndef generate_image(latent_vector):\n    latent_vector = latent_vector.to(device)\n    with torch.no_grad():\n        generated_image = diffusion_decoder(latent_vector)\n    return generated_image\n\n# Convert Tensor to PIL Image\ndef tensor_to_pil(tensor):\n    tensor = tensor.squeeze().cpu().detach().clamp(0, 1)\n    array = tensor.permute(1, 2, 0).numpy()\n    return Image.fromarray((array * 255).astype(np.uint8))\n\n\ngenerated_image = generate_image(latent_vector)\ngenerated_image_pil = tensor_to_pil(generated_image)\n#generated_image_pil.show()\n# Save the generated image to a file\nimage_path = '/tmp/generated_image.png'\ngenerated_image_pil.save(image_path)\n\nprint(f\"Image saved to {image_path}\")\n\n\n# Display the image in a Jupyter notebook\ndisplay(generated_image_pil)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T09:58:19.606396Z","iopub.execute_input":"2024-08-22T09:58:19.607127Z","iopub.status.idle":"2024-08-22T09:58:19.658114Z","shell.execute_reply.started":"2024-08-22T09:58:19.607097Z","shell.execute_reply":"2024-08-22T09:58:19.657256Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Image saved to /tmp/generated_image.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=64x64>","image/png":"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"},"metadata":{}}]}]}