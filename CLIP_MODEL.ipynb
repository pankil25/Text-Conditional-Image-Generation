{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5173923,"sourceType":"datasetVersion","datasetId":3007402},{"sourceId":9192360,"sourceType":"datasetVersion","datasetId":5557058},{"sourceId":9205070,"sourceType":"datasetVersion","datasetId":5565663},{"sourceId":9209952,"sourceType":"datasetVersion","datasetId":5568917},{"sourceId":9211475,"sourceType":"datasetVersion","datasetId":5569939}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport clip\nimport json\nfrom PIL import Image\nimport os\nimport hashlib\nimport urllib\nimport warnings\nfrom packaging import version\nfrom typing import Union, List , Tuple\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nimport gzip\nimport html\nfrom functools import lru_cache\nimport ftfy\nimport regex as re\n\n\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC\n\n\nif version.parse(torch.__version__) < version.parse(\"1.7.1\"):\n    warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:30:39.535835Z","iopub.execute_input":"2024-08-20T14:30:39.536226Z","iopub.status.idle":"2024-08-20T14:30:39.544023Z","shell.execute_reply.started":"2024-08-20T14:30:39.536198Z","shell.execute_reply":"2024-08-20T14:30:39.543099Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# **CLIP Model Methods:**","metadata":{}},{"cell_type":"markdown","source":"**MODEL CODE:**","metadata":{}},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.relu2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x[:1], key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n        return x.squeeze(0)\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.input_resolution = input_resolution\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        def stem(x):\n            x = self.relu1(self.bn1(self.conv1(x)))\n            x = self.relu2(self.bn2(self.conv2(x)))\n            x = self.relu3(self.bn3(self.conv3(x)))\n            x = self.avgpool(x)\n            return x\n\n        x = x.type(self.conv1.weight.dtype)\n        x = stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)\n\n        self.transformer = Transformer(width, layers, heads)\n\n        self.ln_post = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, 0, :])\n\n        if self.proj is not None:\n            x = x @ self.proj\n\n        return x\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,\n                 vocab_size: int,\n                 transformer_width: int,\n                 transformer_heads: int,\n                 transformer_layers: int\n                 ):\n        super().__init__()\n\n        self.context_length = context_length\n\n        if isinstance(vision_layers, (tuple, list)):\n            vision_heads = vision_width * 32 // 64\n            self.visual = ModifiedResNet(\n                layers=vision_layers,\n                output_dim=embed_dim,\n                heads=vision_heads,\n                input_resolution=image_resolution,\n                width=vision_width\n            )\n        else:\n            vision_heads = vision_width // 64\n            self.visual = VisionTransformer(\n                input_resolution=image_resolution,\n                patch_size=vision_patch_size,\n                width=vision_width,\n                layers=vision_layers,\n                heads=vision_heads,\n                output_dim=embed_dim\n            )\n\n        self.transformer = Transformer(\n            width=transformer_width,\n            layers=transformer_layers,\n            heads=transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = vocab_size\n        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n        self.ln_final = LayerNorm(transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        if isinstance(self.visual, ModifiedResNet):\n            if self.visual.attnpool is not None:\n                std = self.visual.attnpool.c_proj.in_features ** -0.5\n                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n\n            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n                for name, param in resnet_block.named_parameters():\n                    if name.endswith(\"bn3.weight\"):\n                        nn.init.zeros_(param)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    @property\n    def dtype(self):\n        return self.visual.conv1.weight.dtype\n\n    def encode_image(self, image):\n        return self.visual(image.type(self.dtype))\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x).type(self.dtype)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # normalized features\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n\n        # cosine similarity as logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n\n        # shape = [global_batch_size, global_batch_size]\n        return logits_per_image, logits_per_text\n\n\ndef convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n\n\ndef build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_resolution = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n\n    model = CLIP(\n        embed_dim,\n        image_resolution, vision_layers, vision_width, vision_patch_size,\n        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        if key in state_dict:\n            del state_dict[key]\n\n    convert_weights(model)\n    model.load_state_dict(state_dict)\n    return model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:30:40.368117Z","iopub.execute_input":"2024-08-20T14:30:40.368429Z","iopub.status.idle":"2024-08-20T14:30:40.448620Z","shell.execute_reply.started":"2024-08-20T14:30:40.368406Z","shell.execute_reply":"2024-08-20T14:30:40.447726Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"**TOKENIZER CODE:**","metadata":{}},{"cell_type":"code","source":"File_directory=\"/kaggle/input/bpe-simple-vocab-16e6-txt-zip\"\n@lru_cache()\ndef default_bpe():\n    return os.path.join(File_directory,\"bpe_simple_vocab_16e6.txt\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n          # Open and read the BPE file correctly\n        with open(bpe_path, \"r\", encoding=\"utf-8\") as file:\n            merges = file.read().split('\\n')\n#          merges = (bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:40:24.684940Z","iopub.execute_input":"2024-08-20T14:40:24.685291Z","iopub.status.idle":"2024-08-20T14:40:24.710821Z","shell.execute_reply.started":"2024-08-20T14:40:24.685264Z","shell.execute_reply":"2024-08-20T14:40:24.709927Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"**CLIP MODEL METHODS:**","metadata":{}},{"cell_type":"code","source":"__all__ = [\"available_models\", \"load\", \"tokenize\"]\n_tokenizer = SimpleTokenizer()\n\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n}\n\n\ndef _download(url: str, root: str):\n    os.makedirs(root, exist_ok=True)\n    filename = os.path.basename(url)\n\n    expected_sha256 = url.split(\"/\")[-2]\n    download_target = os.path.join(root, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n            return download_target\n        else:\n            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n        raise RuntimeError(\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef _convert_image_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\ndef _transform(n_px):\n    return Compose([\n        Resize(n_px, interpolation=BICUBIC),\n        CenterCrop(n_px),\n        _convert_image_to_rgb,\n        ToTensor(),\n        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n    ])\n\n\ndef available_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list(_MODELS.keys())\n\n\ndef load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n\n    device : Union[str, torch.device]\n        The device to put the loaded model\n\n    jit : bool\n        Whether to load the optimized JIT model or more hackable non-JIT model (default).\n\n    download_root: str\n        path to download the model files; by default, it uses \"~/.cache/clip\"\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if name in _MODELS:\n        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n\n    with open(model_path, 'rb') as opened_file:\n        try:\n            # loading JIT archive\n            model = torch.jit.load(opened_file, map_location=device if jit else \"cpu\").eval()\n            state_dict = None\n        except RuntimeError:\n            # loading saved state dict\n            if jit:\n                warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n                jit = False\n            state_dict = torch.load(opened_file, map_location=\"cpu\")\n\n    if not jit:\n        model = build_model(state_dict or model.state_dict()).to(device)\n        if str(device) == \"cpu\":\n            model.float()\n        return model, _transform(model.visual.input_resolution)\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def _node_get(node: torch._C.Node, key: str):\n        \"\"\"Gets attributes of a node which is polymorphic over return type.\n        \n        From https://github.com/pytorch/pytorch/pull/82628\n        \"\"\"\n        sel = node.kindOf(key)\n        return getattr(node, sel)(key)\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(_node_get(node, \"value\")).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 on CPU\n    if str(device) == \"cpu\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if _node_get(inputs[i].node(), \"value\") == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n\n        model.float()\n\n    return model, _transform(model.input_resolution.item())\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    truncate: bool\n        Whether to truncate the text in case its encoding is longer than the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    if version.parse(torch.__version__) < version.parse(\"1.8.0\"):\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n    else:\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            if truncate:\n                tokens = tokens[:context_length]\n                tokens[-1] = eot_token\n            else:\n                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n        result[i, :len(tokens)] = torch.tensor(tokens)\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:40:35.031011Z","iopub.execute_input":"2024-08-20T14:40:35.031451Z","iopub.status.idle":"2024-08-20T14:40:35.329050Z","shell.execute_reply.started":"2024-08-20T14:40:35.031416Z","shell.execute_reply":"2024-08-20T14:40:35.328059Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# **CLIP EMBEDDINGS:**","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = load(\"ViT-B/32\", device=device)\n\n\n# Load COCO dataset\ndata_dir = '/kaggle/input'  # Update this to your COCO dataset path\ndata_dir_2='/kaggle/input/whuewi'\nannotation_file = os.path.join(data_dir_2, 'caption.json')\n\nwith open(annotation_file, 'r') as f:\n    coco_annotations = json.load(f)\n\n# Directory containing images\nimage_dir = '/kaggle/input/val-dummy/'  # Update this to your images directory\n\n# Process each image and its captions\nfor image_info in coco_annotations['images']:\n    image_id = image_info['id']\n    image_file = image_info['file_name']\n    image_path = os.path.join(image_dir, image_file)\n\n    # Preprocess the image\n    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n \n    \n    # Get captions for this image\n    captions = [ann['caption'] for ann in coco_annotations['annotations'] if ann['image_id'] == image_id]\n    \n    # Tokenize captions\n    text = tokenize(captions).to(device)\n    \n    # Compute image and text features\n    with torch.no_grad():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n        \n#         # Compute similarity\n#         logits_per_image, logits_per_text = model(image, text)\n#         probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n    \n#     # Print results for this image\n#     print(f\"Image ID: {image_id}, Label probs: {probs}\")\n    # Print the image and text embeddings\n#     print(f\"Image ID: {image_id}\")\n#     print(f\"Image Embeddings: {image_features}\")\n#     print(f\"Text Embeddings: {text_features}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T14:40:53.643852Z","iopub.execute_input":"2024-08-20T14:40:53.644692Z","iopub.status.idle":"2024-08-20T14:40:58.278609Z","shell.execute_reply.started":"2024-08-20T14:40:53.644656Z","shell.execute_reply":"2024-08-20T14:40:58.277592Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Image ID: 42\nImage Embeddings: tensor([[ 1.0419e-01,  1.2317e-01, -4.4312e-01,  5.5029e-01,  2.4316e-01,\n          8.5449e-02, -1.8091e-01, -2.8320e-02, -1.8030e-01, -7.7942e-02,\n          1.3062e-01, -1.9714e-01, -5.4932e-02, -3.2788e-01, -1.7053e-01,\n          3.3838e-01,  4.0015e-01,  2.2156e-02,  5.0354e-02, -3.2983e-01,\n         -8.4912e-01, -9.9976e-02,  2.9883e-01, -2.3938e-01,  5.6104e-01,\n          5.0586e-01,  5.6445e-01, -1.1145e-01,  1.7004e-01,  1.4319e-01,\n         -2.2717e-01,  3.9062e-01, -1.2085e-01, -2.6172e-01, -4.1772e-01,\n          1.2622e-01,  5.1904e-01, -2.7222e-01, -3.2642e-01,  4.9365e-01,\n         -2.5830e-01,  2.9932e-01, -1.8347e-01, -2.1350e-01,  2.4072e-01,\n         -5.7080e-01, -5.3223e-01,  2.8638e-01, -4.2603e-01, -7.9712e-02,\n          6.6504e-01, -3.2373e-01, -3.7781e-02,  2.6953e-01,  1.6785e-02,\n          2.0264e-01,  3.9697e-01, -1.4966e-01, -6.6064e-01, -1.7126e-01,\n          5.8203e-01, -2.4673e-02,  3.1372e-01,  3.7891e-01, -1.9312e-01,\n         -1.9604e-01,  5.0635e-01,  1.6572e+00, -8.3618e-02,  9.9670e-02,\n          4.5703e-01, -1.1230e-02,  3.0298e-01, -1.5820e-01,  5.7129e-01,\n         -1.5381e-01, -2.3987e-01, -5.1758e-01, -4.9341e-01, -5.8203e-01,\n         -3.2861e-01,  1.5662e-01, -1.8701e-01, -2.9883e-01,  1.6064e-01,\n         -8.6182e-02,  3.4497e-01, -2.0667e-01,  3.3765e-01, -2.2717e-01,\n         -3.1787e-01, -7.3051e-03, -6.5469e+00,  2.8711e-01, -5.4834e-01,\n         -3.8872e-03, -2.0056e-01,  1.5271e-01, -3.0835e-01,  1.0605e+00,\n          3.2690e-01, -1.9434e-01,  3.7964e-01, -4.7699e-02,  4.9097e-01,\n         -8.9294e-02,  6.8848e-01, -1.9995e-01,  1.3382e-02, -1.9055e-01,\n         -2.4036e-01, -1.7542e-01, -1.9849e-01, -2.7954e-01, -9.7534e-02,\n          2.2717e-01,  1.7444e-01, -1.1108e-01,  5.2881e-01, -2.6172e-01,\n          2.1881e-02, -1.3306e-01,  1.7236e-01,  4.7510e-01, -1.0913e-01,\n         -5.0830e-01, -4.5581e-01,  1.1847e-01,  1.5771e-01,  3.1982e-01,\n          1.7249e-01, -9.7900e-02, -1.0571e-01,  9.1846e-01, -4.9976e-01,\n         -3.2495e-01, -5.0781e-02, -5.0049e-01, -2.0471e-01, -1.2146e-02,\n         -1.7090e-01, -1.8872e-01,  9.1614e-02, -1.5735e-01, -4.1168e-02,\n          2.1338e-01, -1.7773e-01, -7.9395e-01,  1.1713e-01,  3.8452e-01,\n         -1.3757e-01,  3.1201e-01, -1.3662e+00, -7.0557e-02, -2.5330e-02,\n         -4.5410e-02,  7.4501e-03, -9.9243e-02,  5.5566e-01, -7.4036e-02,\n         -5.7861e-01,  4.1553e-01,  2.5464e-01,  7.9785e-01,  3.2861e-01,\n         -7.0166e-01,  1.9446e-01,  3.4644e-01,  2.5928e-01,  3.0469e-01,\n          2.9053e-01,  1.2421e-02, -8.9844e-02, -1.4282e-01, -1.7163e-01,\n         -2.1088e-02,  9.7803e-01,  5.7031e-01, -1.7358e-01, -3.3630e-02,\n         -3.5742e-01,  2.4146e-01, -1.8066e-01, -2.7417e-01, -5.6076e-03,\n         -1.0300e-02,  1.8762e-01, -1.4307e-01, -1.5442e-01,  1.2939e-01,\n          1.5466e-01,  1.1731e-01, -2.2925e-01,  5.6244e-02, -7.2510e-01,\n         -3.0981e-01,  3.9844e-01, -8.0139e-02, -8.5303e-01, -5.3027e-01,\n          2.1729e-01,  3.3032e-01,  6.4087e-02,  5.2930e-01, -4.1113e-01,\n         -9.2529e-02, -4.8193e-01, -8.1238e-02, -1.9165e-01, -7.2021e-02,\n         -4.7876e-01, -3.5449e-01,  8.1543e-02, -1.0980e-01, -1.2097e-01,\n          9.8877e-02,  4.2651e-01, -1.0712e-01,  7.1924e-01,  1.0565e-01,\n          1.8530e-01,  2.5366e-01,  3.5791e-01,  6.8420e-02, -4.5264e-01,\n         -1.0706e-01, -2.7710e-01, -2.3767e-01,  2.9346e-01,  6.6992e-01,\n          7.1350e-02, -2.6566e-02,  4.0234e-01,  2.6123e-01,  6.4514e-02,\n         -4.7241e-01, -1.6748e-01, -2.8931e-01, -2.9938e-02,  7.0374e-02,\n         -3.1311e-02,  1.2500e-01,  3.3325e-02,  3.5669e-01, -1.2384e-01,\n          1.0469e+00,  2.5806e-01, -1.8787e-01,  4.5090e-03,  4.2969e-01,\n         -1.2183e-01, -7.7087e-02,  5.2930e-01, -6.4880e-02, -5.0049e-02,\n          3.0029e-01,  4.6783e-02,  2.4390e-01, -6.8054e-02, -1.5552e-01,\n          5.0928e-01,  2.1375e-01, -4.7882e-02,  1.2812e+00,  1.2433e-01,\n          3.7567e-02, -3.5229e-01,  2.5244e-01, -2.3462e-01,  2.5269e-02,\n         -3.9648e-01, -2.6709e-01,  2.0850e-01,  1.3745e-01, -8.9661e-02,\n          2.1619e-01,  1.8555e-01, -7.2144e-02,  2.5000e-01,  1.1816e-01,\n          1.8787e-01,  4.3384e-01, -2.4060e-01, -6.6504e-01,  1.6309e-01,\n         -7.8064e-02,  3.0322e-01, -4.4995e-01, -2.1667e-01, -3.2935e-01,\n          4.6448e-02, -5.8411e-02,  7.9285e-02,  2.8052e-01,  1.9080e-01,\n          1.0797e-01,  1.4417e-01, -5.3894e-02, -3.8177e-02,  5.4980e-01,\n         -2.9312e-02, -1.7212e-01, -6.0010e-01,  4.7876e-01,  4.4800e-02,\n         -3.2666e-01, -1.9019e-01,  6.7969e-01, -3.4009e-01, -4.6826e-01,\n         -3.7573e-01,  6.5723e-01,  9.1699e-01,  3.4576e-02, -5.5695e-02,\n          3.0811e-01,  3.4375e-01,  3.9990e-01,  4.9622e-02, -5.2002e-01,\n          6.2842e-01,  2.5244e-01, -3.8403e-01, -9.7961e-02,  1.3550e-01,\n         -1.8921e-01,  1.7554e-01,  1.4893e-01, -2.5464e-01, -8.4305e-03,\n          3.9502e-01,  1.1176e-01, -3.0823e-02, -2.6416e-01, -1.2396e-01,\n         -4.2145e-02,  1.5515e-01,  4.3335e-02,  1.9421e-01,  2.4097e-01,\n         -1.4404e-01, -2.9932e-01, -1.0193e-01, -1.8469e-01, -5.4352e-02,\n          3.2007e-01, -3.8257e-01,  9.5276e-02,  6.2012e-02,  2.6245e-01,\n         -4.5703e-01, -1.5060e-02,  5.2734e-01,  6.1615e-02,  1.9397e-01,\n          1.7981e-01, -4.8608e-01,  1.8701e-01,  1.0126e-01,  2.9883e-01,\n          3.0542e-01, -1.9165e-01, -2.1936e-01,  1.5283e-01, -8.2520e-02,\n          3.5962e-01,  9.5337e-02, -3.4692e-01,  4.9756e-01,  8.4412e-02,\n         -5.3369e-01,  9.5398e-02,  9.5520e-02,  2.9663e-02,  3.6011e-01,\n          1.4868e-01,  1.4971e+00, -6.0730e-02, -4.8706e-01,  1.1490e-02,\n          2.2049e-02, -2.3743e-01, -1.3000e-01, -2.3328e-01, -2.7612e-01,\n         -3.9722e-01, -2.2742e-01, -3.0615e-01,  3.9136e-01, -7.6709e-01,\n         -9.2822e-01, -4.6387e-01, -2.6196e-01,  1.3452e-01, -1.4380e-01,\n         -5.2832e-01,  1.5259e-01,  1.6528e-01, -2.8345e-01, -1.8530e-01,\n         -2.7246e-01, -9.9304e-02,  1.3691e+00,  1.1578e-01,  1.6882e-01,\n         -2.2852e-01,  3.7903e-02,  4.1235e-01,  2.9388e-02, -4.8389e-01,\n          6.5479e-01, -1.1276e-02, -3.3862e-01,  4.4189e-01, -1.3818e-01,\n         -3.4393e-02,  2.7197e-01, -4.1284e-01, -1.5967e-01, -3.9209e-01,\n         -1.4526e-01,  5.0732e-01,  2.2742e-01,  5.5762e-01,  3.0960e-02,\n         -2.8418e-01, -1.3237e-02, -1.1298e-01,  8.4521e-01, -6.9824e-02,\n         -7.2314e-01,  6.0352e-01, -2.8491e-01,  3.1201e-01, -8.7341e-02,\n          9.1980e-02, -3.4106e-01,  5.3613e-01, -6.1829e-02, -1.7334e-01,\n         -6.2622e-02, -2.2583e-01,  3.2153e-01, -3.9575e-01, -3.6041e-02,\n         -1.0956e-02, -1.9836e-01, -3.8452e-01, -3.1641e-01, -2.3669e-01,\n         -6.1279e-01, -4.4287e-01, -2.4986e-03,  2.3328e-01, -1.6064e-01,\n         -1.1334e-01,  3.6206e-01, -1.9324e-01,  1.4636e-01,  3.4326e-01,\n         -1.6296e-01,  4.4897e-01,  4.6558e-01, -2.8931e-01,  3.1763e-01,\n         -2.4304e-01, -5.6641e-01, -2.2842e-02, -8.4619e-01, -1.5210e-01,\n         -7.2852e-01, -2.2314e-01, -1.6577e-01, -6.0352e-01, -1.6602e-01,\n          1.0858e-01, -1.8372e-01,  2.5635e-01,  9.0637e-02,  7.0251e-02,\n          2.8906e-01,  1.0353e-02, -5.2832e-01,  3.9014e-01, -1.2085e-01,\n          1.5833e-01, -4.5776e-01,  5.8105e-01,  1.5759e-01, -1.3342e-01,\n         -9.0454e-02, -4.8291e-01,  4.5239e-01, -7.3779e-01,  3.4302e-01,\n         -1.4380e-01,  3.6572e-01,  2.2144e-01, -1.6479e-01, -7.4524e-02,\n         -4.8315e-01,  4.6661e-02, -3.9502e-01, -2.1216e-01,  1.4685e-01,\n          1.3330e-01, -2.4597e-01, -1.1642e-02,  2.0435e-01,  8.9062e-01,\n          1.3794e-01, -2.4988e-01]], device='cuda:0', dtype=torch.float16)\nText Embeddings: tensor([[-0.3293, -0.0374, -0.1898,  ..., -0.0072,  0.0269,  0.2349],\n        [-0.0674,  0.0453, -0.2983,  ..., -0.1874,  0.1910, -0.1584]],\n       device='cuda:0', dtype=torch.float16)\nImage ID: 73\nImage Embeddings: tensor([[-2.3132e-01,  2.2534e-01, -2.0325e-02,  1.3074e-01, -2.5293e-01,\n         -7.4341e-02,  1.9458e-01, -2.2424e-01,  7.4402e-02,  4.9072e-02,\n          5.4297e-01,  8.7830e-02,  2.1759e-02,  1.2769e-01,  6.8555e-01,\n         -4.9377e-02,  1.0664e+00,  4.3750e-01, -2.3511e-01, -3.6328e-01,\n          5.9521e-01,  5.8075e-02,  2.0544e-01, -4.8291e-01,  5.8868e-02,\n          7.4768e-02, -3.0371e-01, -3.0884e-01, -8.0566e-03, -3.6652e-02,\n         -2.5244e-01,  7.4524e-02, -2.8345e-01,  4.2725e-01, -1.5625e-01,\n          2.9004e-01,  1.8042e-01, -6.1157e-02,  6.0205e-01,  1.4219e+00,\n         -2.9053e-01, -4.1870e-01, -3.8208e-02, -3.2129e-01,  3.1177e-01,\n         -2.5918e+00, -1.9775e-01, -4.9591e-03, -7.7087e-02,  5.0171e-02,\n          3.5010e-01,  6.0791e-02,  4.9609e-01,  2.0593e-01, -1.6064e-01,\n         -1.0358e-01,  4.4238e-01,  2.2021e-01, -2.2119e-01, -6.2207e-01,\n          9.3604e-01, -2.6807e-01,  1.2091e-01,  1.3586e-01, -5.9619e-01,\n          8.3618e-02,  5.3271e-01,  3.3472e-01,  2.9395e-01,  1.6943e-01,\n          4.3994e-01, -1.0132e-01,  1.9702e-01,  8.9050e-02,  2.5635e-01,\n         -8.2703e-02,  2.4194e-01, -1.5991e-01,  8.9722e-02, -4.3164e-01,\n          5.2246e-02, -3.4363e-02, -5.9265e-02,  4.3152e-02, -2.0374e-01,\n          5.8301e-01, -6.8896e-01, -3.6438e-02,  5.6836e-01, -1.0382e-01,\n          1.0907e-01,  5.3444e-03, -6.3047e+00,  9.8096e-01,  3.9941e-01,\n          3.4839e-01,  2.0911e-01,  1.2347e-01, -9.1736e-02,  1.3281e-01,\n          2.0117e-01,  4.8071e-01,  1.6998e-02,  2.8296e-01,  7.0264e-01,\n         -2.3645e-01, -1.7217e+00, -1.1670e-01,  3.6304e-01,  1.0486e-01,\n          2.5537e-01, -1.9910e-01, -4.4556e-01,  7.4707e-02,  7.4402e-02,\n         -3.8208e-01, -5.1562e-01, -2.1790e-01,  2.5977e-01, -1.1853e-01,\n          9.0942e-02, -3.2080e-01, -3.2202e-01,  3.5767e-01, -1.4368e-01,\n         -2.7686e-01,  3.1226e-01,  2.9160e-02,  5.8624e-02,  1.8530e-01,\n         -1.9397e-01, -2.1985e-01,  8.9172e-02,  8.1445e-01,  5.4980e-01,\n         -2.8564e-01,  5.4541e-01, -1.0541e-01, -6.3525e-01, -3.0469e-01,\n          2.4561e-01, -8.3435e-02,  1.4610e-02,  1.8396e-01, -4.1211e-01,\n         -2.0996e-01, -1.5527e-01, -3.8910e-02, -3.7476e-01,  1.3684e-01,\n         -2.9150e-01,  1.8176e-01,  1.0205e+00,  8.1177e-03, -3.2153e-01,\n         -1.8323e-01,  3.3594e-01, -4.4116e-01,  6.4453e-02, -3.8037e-01,\n         -1.1011e-01, -5.5225e-01,  2.2668e-01, -7.5989e-02,  1.3770e-01,\n          1.5869e-01,  6.0596e-01,  6.7969e-01, -5.3955e-02, -2.7856e-01,\n         -4.2285e-01,  9.3018e-02, -5.2643e-02,  1.8127e-01,  5.5176e-02,\n         -2.5049e-01,  1.6191e+00, -9.6512e-04,  2.6538e-01,  6.1963e-01,\n          1.4331e-01, -2.0361e-01,  2.1838e-01, -1.7346e-01, -2.6660e-01,\n          4.3213e-01,  7.0686e-03, -6.8726e-02, -1.8408e-01,  6.2103e-03,\n          2.5806e-01,  3.7378e-01,  1.1774e-01, -2.0288e-01, -7.0020e-01,\n         -4.4922e-02,  9.1492e-02, -9.7229e-02, -8.6865e-01,  1.9800e-01,\n          4.2212e-01, -2.6221e-01,  3.7207e-01,  4.7192e-01, -1.1786e-01,\n          2.1997e-01,  1.1591e-01, -4.2114e-01, -5.3418e-01,  9.3079e-03,\n          4.8438e-01, -4.1064e-01, -2.5732e-01, -3.2080e-01, -3.7671e-01,\n          1.7798e-01,  1.8445e-01,  4.8218e-02, -8.2861e-01, -1.1633e-01,\n         -4.2993e-01, -4.9438e-01, -2.4585e-01, -6.2744e-01,  5.3516e-01,\n         -4.0283e-02, -1.1646e-01, -7.2388e-02,  4.0527e-01, -3.7695e-01,\n         -1.9385e-01, -2.6270e-01,  2.9590e-01,  6.9873e-01,  2.2205e-01,\n          4.2114e-01, -4.8145e-01, -4.3726e-01,  4.9225e-02, -6.1572e-01,\n          5.0293e-01,  1.0327e-01, -3.1885e-01, -8.2129e-01,  7.3509e-03,\n          4.1650e-01,  3.6133e-01,  2.8833e-01,  2.9492e-01,  1.4233e-01,\n          6.4453e-02,  5.1416e-01, -1.1981e-01, -1.4075e-01, -3.6719e-01,\n          9.6741e-02, -2.5342e-01,  2.2498e-01,  1.4197e-01, -2.4304e-01,\n          2.2913e-01, -3.1006e-01,  1.4819e-01,  1.5273e+00,  6.8457e-01,\n         -3.1042e-04, -8.2947e-02, -8.5571e-02,  2.5537e-01,  8.0017e-02,\n         -4.1528e-01,  4.6484e-01,  4.4653e-01, -1.8127e-01,  3.8965e-01,\n          1.6455e-01, -3.5913e-01,  3.0249e-01, -5.2002e-01, -4.8523e-03,\n          4.6558e-01, -2.0715e-01, -6.1572e-01, -2.0618e-01,  2.7222e-01,\n         -3.1274e-01,  5.4443e-01,  9.3750e-02,  5.3680e-02, -4.4580e-01,\n         -1.0095e-01,  1.4917e-01, -7.4768e-02,  1.8982e-01, -1.5732e-02,\n          4.8828e-01,  1.2476e-01,  2.8174e-01, -1.3940e-01,  1.0242e-01,\n         -7.0381e-03,  4.4861e-02, -3.5767e-01,  2.9541e-01,  8.7219e-02,\n          2.1716e-01, -1.1450e-01,  2.5562e-01,  1.4412e-02, -2.4561e-01,\n          1.0840e-01,  3.8916e-01,  8.1104e-01, -7.1533e-02,  3.8623e-01,\n          9.5886e-02, -2.6440e-01,  4.7583e-01,  1.3477e-01,  6.2134e-02,\n          4.3262e-01,  7.8369e-01,  1.0114e-01, -4.3823e-01, -2.5317e-01,\n         -7.7637e-02, -1.0345e-01,  3.2788e-01, -1.0120e-01, -2.7051e-01,\n          5.1758e-01,  3.0396e-01,  2.4017e-02,  1.1407e-01,  4.1046e-02,\n          2.2229e-01,  1.1633e-01, -2.4512e-01, -1.5527e-01,  4.6191e-01,\n         -1.7334e-01, -1.8018e-01,  7.1716e-03, -2.7295e-01,  2.3547e-01,\n         -7.6233e-02, -6.5234e-01, -1.5625e-01,  1.9397e-01,  2.6416e-01,\n         -6.2305e-01, -3.3569e-01,  2.2961e-01, -7.8247e-02, -1.9745e-02,\n          1.7468e-01, -3.6255e-01,  1.1023e-01, -4.9756e-01,  1.1276e-02,\n         -6.3623e-01,  2.5537e-01,  1.7786e-01, -2.6025e-01, -8.2422e-01,\n          6.5613e-02,  2.5781e-01, -1.3440e-01,  3.8025e-02,  1.7773e-01,\n         -8.7967e-03, -1.9775e-01,  1.9592e-02, -3.4961e-01, -1.0486e-01,\n          6.1371e-02,  1.5996e+00,  1.5869e-01,  4.9292e-01,  1.7004e-01,\n         -3.6084e-01,  1.0162e-02,  1.1914e-01,  1.0510e-01, -1.1639e-01,\n         -8.6243e-02,  1.8384e-01,  5.8807e-02, -1.2756e-01,  2.7881e-01,\n         -2.1216e-01,  2.0422e-01, -3.5571e-01, -9.0332e-02, -1.7029e-01,\n          3.3325e-02,  1.4905e-01,  4.8920e-02,  3.3301e-01, -4.9988e-02,\n          1.1670e-01, -6.2042e-02, -2.2363e-01, -1.6858e-01, -8.4045e-02,\n         -1.0765e-02,  4.9561e-01,  3.2178e-01,  8.8965e-01, -3.2104e-01,\n         -6.9336e-02,  2.6172e-01,  7.5623e-02,  2.7002e-01, -1.3062e-01,\n         -6.9873e-01, -3.3447e-01, -3.7085e-01,  1.9263e-01, -3.6475e-01,\n          2.1057e-02, -2.6562e-01,  3.1177e-01, -3.8037e-01, -3.5791e-01,\n         -2.2375e-01,  2.7893e-02,  2.2510e-01,  6.6992e-01,  3.0933e-01,\n         -7.4561e-01,  3.7524e-01,  6.2549e-01,  9.2957e-02,  4.4751e-01,\n         -2.2937e-01, -5.8887e-01,  5.9912e-01, -6.7322e-02,  2.5635e-01,\n         -8.1711e-03,  2.1008e-01,  4.4775e-01, -2.2473e-01, -2.8467e-01,\n         -3.3350e-01, -1.5613e-01, -1.9421e-01,  1.7688e-01,  4.6436e-01,\n         -5.4053e-01, -5.9521e-01, -1.6675e-01,  3.0685e-02, -7.4646e-02,\n          4.5593e-02, -7.8003e-02, -7.1167e-02, -3.3887e-01,  5.8022e-03,\n         -1.3098e-01, -1.5625e-01, -2.7734e-01, -3.4302e-01,  5.1953e-01,\n          4.9658e-01,  1.0410e+00, -4.6045e-01, -1.9995e-01, -3.5767e-01,\n          5.0342e-01, -2.2021e-01,  4.4067e-01, -4.5459e-01, -1.9934e-01,\n          1.4328e-02,  5.3345e-02,  7.3120e-02, -1.6223e-01,  1.7834e-01,\n         -4.1309e-01,  3.0347e-01,  1.1523e-01,  3.7207e-01,  2.7148e-01,\n          4.4067e-02, -3.0493e-01,  4.6655e-01,  1.1182e-01,  3.0981e-01,\n         -3.8452e-01,  2.0398e-01, -3.8261e-03, -7.2363e-01, -1.3562e-01,\n         -4.7876e-01,  6.6833e-02,  1.3062e-02, -2.8833e-01, -2.7246e-01,\n         -4.6753e-01, -5.0879e-01,  1.6772e-01,  3.0640e-01,  7.8760e-01,\n          3.7079e-02, -3.8391e-02, -4.2529e-01,  9.2087e-03,  2.9883e-01,\n          8.6853e-02,  6.5735e-02]], device='cuda:0', dtype=torch.float16)\nText Embeddings: tensor([[ 0.1975, -0.4045, -0.3337,  ...,  0.4673, -0.2815, -0.1145],\n        [ 0.2600, -0.2788,  0.1771,  ..., -0.2966, -0.6318, -0.2416]],\n       device='cuda:0', dtype=torch.float16)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}