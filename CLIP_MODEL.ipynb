{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c2d093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T17:00:19.972852Z",
     "iopub.status.busy": "2024-09-04T17:00:19.971666Z",
     "iopub.status.idle": "2024-09-04T17:00:35.132460Z",
     "shell.execute_reply": "2024-09-04T17:00:35.131239Z"
    },
    "papermill": {
     "duration": 15.172426,
     "end_time": "2024-09-04T17:00:35.134994",
     "exception": false,
     "start_time": "2024-09-04T17:00:19.962568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\r\n",
      "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\r\n",
      "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m633.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ftfy\r\n",
      "Successfully installed ftfy-6.2.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40cf8cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T17:00:35.154164Z",
     "iopub.status.busy": "2024-09-04T17:00:35.153360Z",
     "iopub.status.idle": "2024-09-04T17:00:40.629213Z",
     "shell.execute_reply": "2024-09-04T17:00:40.628342Z"
    },
    "papermill": {
     "duration": 5.487993,
     "end_time": "2024-09-04T17:00:40.631816",
     "exception": false,
     "start_time": "2024-09-04T17:00:35.143823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "import hashlib\n",
    "import urllib\n",
    "import warnings\n",
    "from packaging import version\n",
    "from typing import Union, List , Tuple\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import gzip\n",
    "import html\n",
    "from functools import lru_cache\n",
    "import ftfy\n",
    "import regex as re\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "\n",
    "if version.parse(torch.__version__) < version.parse(\"1.7.1\"):\n",
    "    warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a89025",
   "metadata": {
    "papermill": {
     "duration": 0.00838,
     "end_time": "2024-09-04T17:00:40.649263",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.640883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CLIP Model Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194a56a",
   "metadata": {
    "papermill": {
     "duration": 0.008145,
     "end_time": "2024-09-04T17:00:40.665954",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.657809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**MODEL CODE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b084df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T17:00:40.685451Z",
     "iopub.status.busy": "2024-09-04T17:00:40.684655Z",
     "iopub.status.idle": "2024-09-04T17:00:40.894462Z",
     "shell.execute_reply": "2024-09-04T17:00:40.893366Z"
    },
    "papermill": {
     "duration": 0.222692,
     "end_time": "2024-09-04T17:00:40.897099",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.674407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x[:1], key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "        return x.squeeze(0)\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            x = self.relu1(self.bn1(self.conv1(x)))\n",
    "            x = self.relu2(self.bn2(self.conv2(x)))\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "\n",
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(l):\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.half()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.half()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.half()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr = getattr(l, name)\n",
    "                if attr is not None:\n",
    "                    attr.data = attr.data.half()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)\n",
    "\n",
    "\n",
    "def build_model(state_dict: dict):\n",
    "    vit = \"visual.proj\" in state_dict\n",
    "\n",
    "    if vit:\n",
    "        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        image_resolution = vision_patch_size * grid_size\n",
    "    else:\n",
    "        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
    "        vision_layers = tuple(counts)\n",
    "        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        vision_patch_size = None\n",
    "        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "        image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
    "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
    "\n",
    "    model = CLIP(\n",
    "        embed_dim,\n",
    "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n",
    "    )\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "\n",
    "    convert_weights(model)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e3a67",
   "metadata": {
    "papermill": {
     "duration": 0.00826,
     "end_time": "2024-09-04T17:00:40.914387",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.906127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**TOKENIZER CODE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9348846c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T17:00:40.933172Z",
     "iopub.status.busy": "2024-09-04T17:00:40.932747Z",
     "iopub.status.idle": "2024-09-04T17:00:40.962080Z",
     "shell.execute_reply": "2024-09-04T17:00:40.961043Z"
    },
    "papermill": {
     "duration": 0.041565,
     "end_time": "2024-09-04T17:00:40.964498",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.922933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "File_directory=\"/kaggle/input/bpe-simple-vocab-16e6-txt-zip\"\n",
    "@lru_cache()\n",
    "def default_bpe():\n",
    "    return os.path.join(File_directory,\"bpe_simple_vocab_16e6.txt\")\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = default_bpe()):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "          # Open and read the BPE file correctly\n",
    "        with open(bpe_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges = file.read().split('\\n')\n",
    "#          merges = (bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d4599",
   "metadata": {
    "papermill": {
     "duration": 0.008247,
     "end_time": "2024-09-04T17:00:40.981382",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.973135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**CLIP MODEL METHODS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af93122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T17:00:41.000607Z",
     "iopub.status.busy": "2024-09-04T17:00:41.000227Z",
     "iopub.status.idle": "2024-09-04T17:00:41.260292Z",
     "shell.execute_reply": "2024-09-04T17:00:41.259443Z"
    },
    "papermill": {
     "duration": 0.272814,
     "end_time": "2024-09-04T17:00:41.262851",
     "exception": false,
     "start_time": "2024-09-04T17:00:40.990037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "__all__ = [\"available_models\", \"load\", \"tokenize\"]\n",
    "_tokenizer = SimpleTokenizer()\n",
    "\n",
    "_MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
    "    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
    "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
    "}\n",
    "\n",
    "\n",
    "def _download(url: str, root: str):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "\n",
    "    expected_sha256 = url.split(\"/\")[-2]\n",
    "    download_target = os.path.join(root, filename)\n",
    "\n",
    "    if os.path.exists(download_target) and not os.path.isfile(download_target):\n",
    "        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n",
    "            return download_target\n",
    "        else:\n",
    "            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n",
    "        raise RuntimeError(\"Model has been downloaded but the SHA256 checksum does not not match\")\n",
    "\n",
    "    return download_target\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def available_models() -> List[str]:\n",
    "    \"\"\"Returns the names of available CLIP models\"\"\"\n",
    "    return list(_MODELS.keys())\n",
    "\n",
    "\n",
    "def load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n",
    "    \"\"\"Load a CLIP model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n",
    "\n",
    "    device : Union[str, torch.device]\n",
    "        The device to put the loaded model\n",
    "\n",
    "    jit : bool\n",
    "        Whether to load the optimized JIT model or more hackable non-JIT model (default).\n",
    "\n",
    "    download_root: str\n",
    "        path to download the model files; by default, it uses \"~/.cache/clip\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module\n",
    "        The CLIP model\n",
    "\n",
    "    preprocess : Callable[[PIL.Image], torch.Tensor]\n",
    "        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n",
    "    \"\"\"\n",
    "    if name in _MODELS:\n",
    "        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n",
    "    elif os.path.isfile(name):\n",
    "        model_path = name\n",
    "    else:\n",
    "        raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n",
    "\n",
    "    with open(model_path, 'rb') as opened_file:\n",
    "        try:\n",
    "            # loading JIT archive\n",
    "            model = torch.jit.load(opened_file, map_location=device if jit else \"cpu\").eval()\n",
    "            state_dict = None\n",
    "        except RuntimeError:\n",
    "            # loading saved state dict\n",
    "            if jit:\n",
    "                warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n",
    "                jit = False\n",
    "            state_dict = torch.load(opened_file, map_location=\"cpu\")\n",
    "\n",
    "    if not jit:\n",
    "        model = build_model(state_dict or model.state_dict()).to(device)\n",
    "        if str(device) == \"cpu\":\n",
    "            model.float()\n",
    "        return model, _transform(model.visual.input_resolution)\n",
    "\n",
    "    # patch the device names\n",
    "    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n",
    "    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n",
    "\n",
    "    def _node_get(node: torch._C.Node, key: str):\n",
    "        \"\"\"Gets attributes of a node which is polymorphic over return type.\n",
    "        \n",
    "        From https://github.com/pytorch/pytorch/pull/82628\n",
    "        \"\"\"\n",
    "        sel = node.kindOf(key)\n",
    "        return getattr(node, sel)(key)\n",
    "\n",
    "    def patch_device(module):\n",
    "        try:\n",
    "            graphs = [module.graph] if hasattr(module, \"graph\") else []\n",
    "        except RuntimeError:\n",
    "            graphs = []\n",
    "\n",
    "        if hasattr(module, \"forward1\"):\n",
    "            graphs.append(module.forward1.graph)\n",
    "\n",
    "        for graph in graphs:\n",
    "            for node in graph.findAllNodes(\"prim::Constant\"):\n",
    "                if \"value\" in node.attributeNames() and str(_node_get(node, \"value\")).startswith(\"cuda\"):\n",
    "                    node.copyAttributes(device_node)\n",
    "\n",
    "    model.apply(patch_device)\n",
    "    patch_device(model.encode_image)\n",
    "    patch_device(model.encode_text)\n",
    "\n",
    "    # patch dtype to float32 on CPU\n",
    "    if str(device) == \"cpu\":\n",
    "        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n",
    "        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n",
    "        float_node = float_input.node()\n",
    "\n",
    "        def patch_float(module):\n",
    "            try:\n",
    "                graphs = [module.graph] if hasattr(module, \"graph\") else []\n",
    "            except RuntimeError:\n",
    "                graphs = []\n",
    "\n",
    "            if hasattr(module, \"forward1\"):\n",
    "                graphs.append(module.forward1.graph)\n",
    "\n",
    "            for graph in graphs:\n",
    "                for node in graph.findAllNodes(\"aten::to\"):\n",
    "                    inputs = list(node.inputs())\n",
    "                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n",
    "                        if _node_get(inputs[i].node(), \"value\") == 5:\n",
    "                            inputs[i].node().copyAttributes(float_node)\n",
    "\n",
    "        model.apply(patch_float)\n",
    "        patch_float(model.encode_image)\n",
    "        patch_float(model.encode_text)\n",
    "\n",
    "        model.float()\n",
    "\n",
    "    return model, _transform(model.input_resolution.item())\n",
    "\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n",
    "    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    if version.parse(torch.__version__) < version.parse(\"1.8.0\"):\n",
    "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "    else:\n",
    "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3101f",
   "metadata": {
    "papermill": {
     "duration": 0.008246,
     "end_time": "2024-09-04T17:00:41.279613",
     "exception": false,
     "start_time": "2024-09-04T17:00:41.271367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CLIP EMBEDDINGS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f115627b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-04T17:00:41.296574Z",
     "iopub.status.busy": "2024-09-04T17:00:41.296195Z",
     "iopub.status.idle": "2024-09-04T18:44:16.931912Z",
     "shell.execute_reply": "2024-09-04T18:44:16.930899Z"
    },
    "papermill": {
     "duration": 6215.647673,
     "end_time": "2024-09-04T18:44:16.935000",
     "exception": false,
     "start_time": "2024-09-04T17:00:41.287327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 138MiB/s]\n",
      "Processing Images: 100%|██████████| 82783/82783 [1:43:25<00:00, 13.34it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "# Load COCO dataset\n",
    "data_dir_2='/kaggle/input/coco-image-caption/annotations_trainval2014/annotations'\n",
    "annotation_file = os.path.join(data_dir_2, 'captions_train2014.json')\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    coco_annotations = json.load(f)\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = '/kaggle/input/coco-image-caption/train2014/train2014'  # Update this to your images directory\n",
    "\n",
    "text_embeddings=[]\n",
    "image_embeddings=[]\n",
    "\n",
    "\n",
    "# Process each image and its captions\n",
    "for image_info in tqdm(coco_annotations['images'], desc=\"Processing Images\"):\n",
    "    image_id = image_info['id']\n",
    "    image_file = image_info['file_name']\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    " \n",
    "    \n",
    "    # Get captions for this image\n",
    "    captions = [ann['caption'] for ann in coco_annotations['annotations'] if ann['image_id'] == image_id]\n",
    "    \n",
    "    # Tokenize captions\n",
    "    text = tokenize(captions).to(device)\n",
    "    \n",
    "    # Compute image and text features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        \n",
    "    image_embeddings.append(image_features)\n",
    "    text_embeddings.append(text_features)\n",
    "#     print(image_id)\n",
    "#     print(\"hi\")\n",
    "#     print(text.shape)\n",
    "        \n",
    "#         # Compute similarity\n",
    "#         logits_per_image, logits_per_text = model(image, text)\n",
    "#         probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "#     # Print results for this image\n",
    "#     print(f\"Image ID: {image_id}, Label probs: {probs}\")\n",
    "    # Print the image and text embeddings\n",
    "#     print(f\"Image ID: {image_id}\")\n",
    "#     print(f\"Image Embeddings: {image_features}\")\n",
    "#     print(f\"Text Embeddings: {text_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0982629a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:44:23.903338Z",
     "iopub.status.busy": "2024-09-04T18:44:23.902519Z",
     "iopub.status.idle": "2024-09-04T18:45:05.552266Z",
     "shell.execute_reply": "2024-09-04T18:45:05.551240Z"
    },
    "papermill": {
     "duration": 45.070174,
     "end_time": "2024-09-04T18:45:05.554886",
     "exception": false,
     "start_time": "2024-09-04T18:44:20.484712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Assume text_embeddings is a list of tensors\n",
    "#text_embeddings = [torch.randn(5, 512), torch.randn(6, 512)]  # Example tensors\n",
    "\n",
    "def save_tensors(tensor_list, directory):\n",
    "    \"\"\"\n",
    "    Saves a list of tensors to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        tensor_list (list of torch.Tensor): List of tensors to save.\n",
    "        directory (str): Directory where tensors should be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    for idx, tensor in enumerate(tensor_list):\n",
    "        save_path = os.path.join(directory, f'tensor_{idx}.pt')\n",
    "        torch.save(tensor, save_path)\n",
    "        #print(f\"Saved tensor {idx} to {save_path}\")\n",
    "\n",
    "# Example usage\n",
    "save_directory = '/kaggle/working/text_embeddings_stored_2'\n",
    "save_tensors(text_embeddings, save_directory)\n",
    "\n",
    "# Example usage\n",
    "save_directory_2 = '/kaggle/working/image_embeddings_stored_2'\n",
    "save_tensors(image_embeddings, save_directory_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277dcbeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:45:12.327302Z",
     "iopub.status.busy": "2024-09-04T18:45:12.326578Z",
     "iopub.status.idle": "2024-09-04T18:45:12.336196Z",
     "shell.execute_reply": "2024-09-04T18:45:12.335357Z"
    },
    "papermill": {
     "duration": 3.362579,
     "end_time": "2024-09-04T18:45:12.338126",
     "exception": false,
     "start_time": "2024-09-04T18:45:08.975547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_embeddings, text_embeddings):\n",
    "        self.image_embeddings = image_embeddings\n",
    "        self.text_embeddings = text_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_embedding = self.image_embeddings[idx]\n",
    "        text_embedding = self.text_embeddings[idx]\n",
    "        \n",
    "        # Ensure they are tensors\n",
    "        if not isinstance(image_embedding, torch.Tensor):\n",
    "            image_embedding = torch.tensor(image_embedding, dtype=torch.float32)\n",
    "        \n",
    "        if not isinstance(text_embedding, torch.Tensor):\n",
    "            text_embedding = torch.tensor(text_embedding, dtype=torch.float32)\n",
    "        \n",
    "        return text_embedding, image_embedding\n",
    "\n",
    "# Create dataset\n",
    "dataset = CLIPDataset(image_embeddings, text_embeddings)\n",
    "def custom_collate(batch):\n",
    "    text_embeddings, image_embeddings = zip(*batch)\n",
    "    \n",
    "    # Pad text embeddings to have the same length\n",
    "    text_embeddings_padded = pad_sequence(text_embeddings, batch_first=True)\n",
    "    \n",
    "    # Stack image embeddings (assuming they are all the same size)\n",
    "    image_embeddings = torch.stack(image_embeddings, dim=0)\n",
    "    \n",
    "    return text_embeddings_padded, image_embeddings\n",
    "\n",
    "# Use the custom collate function in your DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    collate_fn=custom_collate,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "#dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc6c966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:45:19.228194Z",
     "iopub.status.busy": "2024-09-04T18:45:19.227835Z",
     "iopub.status.idle": "2024-09-04T18:45:19.234183Z",
     "shell.execute_reply": "2024-09-04T18:45:19.233271Z"
    },
    "papermill": {
     "duration": 3.37556,
     "end_time": "2024-09-04T18:45:19.236124",
     "exception": false,
     "start_time": "2024-09-04T18:45:15.860564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Example Prior Model\n",
    "# class PriorModel(nn.Module):\n",
    "#     def __init__(self, input_dim, latent_dim):\n",
    "#         super(PriorModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 512)\n",
    "#         self.fc2 = nn.Linear(512, latent_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize Prior Model\n",
    "# input_dim = 512  # Dimension of CLIP text embedding\n",
    "# latent_dim = 512  # Dimension of latent representation (same as CLIP image embedding)\n",
    "# prior_model = PriorModel(input_dim, latent_dim).to(device)\n",
    "\n",
    "# # Example training loop\n",
    "# optimizer = optim.Adam(prior_model.parameters(), lr=1e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Assuming you have text_embedding (from CLIP) and image_embedding (from CLIP) pairs\n",
    "# num_epochs = 10  # Number of epochs for training\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "    \n",
    "#     for text_embedding, image_embedding in dataloader:\n",
    "#         text_embedding = text_embedding.to(device)\n",
    "#         image_embedding = image_embedding.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         # Ensure the text embedding has the same dtype as the model weights\n",
    "#         text_embedding = text_embedding.to(dtype=prior_model.fc1.weight.dtype)\n",
    "#         image_embedding = image_embedding.to(dtype=prior_model.fc1.weight.dtype)\n",
    "#         # Forward pass through prior model\n",
    "#         latent = prior_model(text_embedding)\n",
    "#         image_embedding = image_embedding.squeeze(1)\n",
    "        \n",
    "\n",
    "\n",
    "#         latent = latent[:, 0, :]\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "#         # Compute loss with respect to actual CLIP image embedding\n",
    "#         loss = criterion(latent, image_embedding)\n",
    "\n",
    "#         # Backpropagation and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "# # Save the trained model\n",
    "# torch.save(prior_model.state_dict(), 'prior_model.pth')\n",
    "\n",
    "# # Inference: Load the trained model and get latent vectors\n",
    "# def get_latent_vector(text_embedding):\n",
    "#     prior_model.eval()  # Set the model to evaluation mode\n",
    "#     text_embedding = text_embedding.to(device)\n",
    "#     text_embedding = text_embedding.to(dtype=prior_model.fc1.weight.dtype)\n",
    "#     with torch.no_grad():\n",
    "#         latent = prior_model(text_embedding)\n",
    "#     return latent\n",
    "\n",
    "# # Example inference\n",
    "# # Assuming you have a new text_embedding tensor for inference\n",
    "# new_text_embedding = torch.randn(1, input_dim).to(device)  # Example tensor\n",
    "# latent_vector = get_latent_vector(new_text_embedding)\n",
    "# print(f'Latent vector : {latent_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0dd689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:45:26.009671Z",
     "iopub.status.busy": "2024-09-04T18:45:26.009270Z",
     "iopub.status.idle": "2024-09-04T18:52:52.585534Z",
     "shell.execute_reply": "2024-09-04T18:52:52.584529Z"
    },
    "papermill": {
     "duration": 452.897251,
     "end_time": "2024-09-04T18:52:55.509199",
     "exception": false,
     "start_time": "2024-09-04T18:45:22.611948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1024, 1, 512])) that is different to the input size (torch.Size([1024, 6, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1024, 1, 512])) that is different to the input size (torch.Size([1024, 5, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1024, 1, 512])) that is different to the input size (torch.Size([1024, 7, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([863, 1, 512])) that is different to the input size (torch.Size([863, 6, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\n",
      "Epochs:  10%|█         | 1/10 [00:44<06:43, 44.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  20%|██        | 2/10 [01:29<05:56, 44.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.2127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  30%|███       | 3/10 [02:13<05:12, 44.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  40%|████      | 4/10 [02:58<04:28, 44.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 1.2135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  50%|█████     | 5/10 [03:43<03:43, 44.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 1.2112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  60%|██████    | 6/10 [04:27<02:58, 44.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 1.2127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  70%|███████   | 7/10 [05:12<02:13, 44.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 1.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([863, 1, 512])) that is different to the input size (torch.Size([863, 5, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\n",
      "Epochs:  80%|████████  | 8/10 [05:56<01:28, 44.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 1.2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  90%|█████████ | 9/10 [06:41<00:44, 44.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 1.2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches:   0%|          | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs: 100%|██████████| 10/10 [07:26<00:00, 44.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 1.2119\n",
      "Latent vector : tensor([[[-0.8998, -0.7030, -0.4340,  ..., -0.6030,  0.2793,  0.2754],\n",
      "         [ 0.9118,  0.6000,  1.7404,  ...,  0.7556, -2.8977,  0.1784],\n",
      "         [-0.1367, -0.5560,  1.5826,  ...,  1.1654, -0.9346, -2.1020],\n",
      "         [ 1.3936, -0.6566, -0.5253,  ...,  0.5242, -0.5193,  1.5400],\n",
      "         [ 0.1976,  1.0694,  1.1243,  ...,  0.7473,  0.1496, -0.7427],\n",
      "         [-1.0855, -2.1031, -0.3919,  ..., -1.3269,  0.2663,  0.6821]],\n",
      "\n",
      "        [[-0.3791,  0.6668,  0.3763,  ..., -0.9579, -1.4058,  0.1589],\n",
      "         [ 0.2237,  0.7306, -1.0794,  ...,  0.1730,  1.3959,  1.0097],\n",
      "         [-1.7224, -0.4040,  0.1461,  ...,  0.5379, -1.0519,  1.6939],\n",
      "         [-0.0842, -0.6727, -0.9293,  ..., -0.1199,  0.0939,  1.1727],\n",
      "         [ 1.3826,  0.5243, -0.4387,  ..., -1.8705,  1.5229,  0.8799],\n",
      "         [ 0.2852, -1.8795, -1.2695,  ...,  0.1948, -0.0128, -1.7636]],\n",
      "\n",
      "        [[ 1.3258, -1.3445,  1.6490,  ..., -0.6555,  1.8053, -0.4184],\n",
      "         [-1.3293,  0.3948,  0.4603,  ..., -0.0694,  1.3409, -0.9533],\n",
      "         [ 0.3705,  0.8333,  0.1736,  ..., -0.1570,  0.5076, -0.7072],\n",
      "         [-0.7520,  0.3427, -1.1324,  ...,  2.2378,  0.8844, -0.8302],\n",
      "         [-0.7991, -1.7649, -1.4948,  ...,  0.8017, -1.6044,  0.1761],\n",
      "         [ 1.3476,  0.9075, -0.6723,  ..., -0.7551,  0.2204,  1.2357]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1688, -0.1137, -0.1112,  ...,  0.7520,  0.1865, -0.0996],\n",
      "         [ 1.1473, -0.9227, -0.5070,  ...,  1.2306, -0.3809,  0.1447],\n",
      "         [-0.8176, -0.9436,  1.3474,  ..., -0.6654,  0.2357, -1.4230],\n",
      "         [ 0.3041,  0.9314, -1.8761,  ..., -0.4194,  1.9409,  0.3980],\n",
      "         [-0.0522,  0.4326,  2.2770,  ...,  0.9691,  1.1945, -1.8938],\n",
      "         [-0.6717, -0.5014,  0.8923,  ..., -1.3715, -1.0225, -0.8029]],\n",
      "\n",
      "        [[-0.6092, -0.6570,  0.2168,  ..., -1.7858,  1.8701, -0.3201],\n",
      "         [-0.7033, -0.4998,  0.4537,  ..., -0.6579, -0.4434, -0.6688],\n",
      "         [ 1.1179, -0.5945, -1.8355,  ...,  0.2272,  0.1936, -1.2886],\n",
      "         [-2.3817, -1.9622,  0.1351,  ..., -2.2430, -0.1097, -1.6356],\n",
      "         [-0.4592,  1.0381, -0.6940,  ...,  0.1226, -0.4181,  0.4263],\n",
      "         [ 0.0787,  2.0871,  0.1571,  ..., -1.1807, -0.9516, -0.5171]],\n",
      "\n",
      "        [[ 0.2939,  1.3992,  1.8204,  ...,  0.4127, -0.0884,  1.0626],\n",
      "         [-1.3843, -0.4492,  0.2281,  ...,  1.4712, -0.4231, -1.4039],\n",
      "         [-0.6380,  0.6213,  1.7413,  ...,  0.5323, -1.1749,  0.7289],\n",
      "         [ 0.3285, -0.9459,  0.8145,  ...,  0.3007,  0.3743, -0.8321],\n",
      "         [ 0.1923,  0.3706, -0.2522,  ...,  1.1012,  0.2089,  0.2002],\n",
      "         [ 0.4356, -1.1010, -0.0318,  ...,  0.1573, -1.2882,  0.0741]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example Prior Model\n",
    "class PriorModel(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(PriorModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Diffusion Prior Model with noise addition\n",
    "class DiffusionPrior(nn.Module):\n",
    "    def __init__(self, prior_model, timesteps: int, beta_start: float = 0.0001, beta_end: float = 0.02):\n",
    "        super(DiffusionPrior, self).__init__()\n",
    "        self.prior_model = prior_model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        # Create the betas for each timestep\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def add_noise(self, x, t):\n",
    "        \"\"\"\n",
    "        Adds noise to the latent vector x at a given timestep t.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x).to(x.device)\n",
    "        alpha_bar_t = self.alpha_bars[t].view(-1, 1)  # Reshape for broadcasting\n",
    "        noisy_x = torch.sqrt(alpha_bar_t) * x + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "        return noisy_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the diffusion model: adds noise at each timestep.\n",
    "        \"\"\"\n",
    "        latent = self.prior_model(x)\n",
    "        for t in range(self.timesteps):\n",
    "            latent = self.add_noise(latent, t)\n",
    "        return latent\n",
    "\n",
    "# Initialize Prior Model\n",
    "input_dim = 512  # Dimension of CLIP text embedding\n",
    "latent_dim = 512  # Dimension of latent representation (same as CLIP image embedding)\n",
    "prior_model = PriorModel(input_dim, latent_dim).to(device)\n",
    "\n",
    "# Initialize Diffusion Prior Model\n",
    "timesteps = 1000\n",
    "diffusion_prior_model = DiffusionPrior(prior_model, timesteps).to(device)\n",
    "\n",
    "# Example training loop\n",
    "optimizer = optim.Adam(diffusion_prior_model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Assuming you have text_embedding (from CLIP) and image_embedding (from CLIP) pairs\n",
    "num_epochs = 10  # Number of epochs for training\n",
    "update_interval=5000\n",
    "new_text_embedding=[]\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    # Inner loop over batches with tqdm\n",
    "#     for text_embedding, image_embedding in tqdm(dataloader, desc=\"Batches\", leave=False):\n",
    "    pbar = tqdm(total=len(dataloader), desc=\"Batches\", leave=False)\n",
    "    \n",
    "    # Inner loop over batches\n",
    "    for i, (text_embedding, image_embedding) in enumerate(dataloader):    \n",
    "        text_embedding = text_embedding.to(device)\n",
    "        image_embedding = image_embedding.to(device)\n",
    "        new_text_embedding.append(text_embedding)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the embeddings have the same dtype as the model weights\n",
    "        text_embedding = text_embedding.to(dtype=prior_model.fc1.weight.dtype)\n",
    "        image_embedding = image_embedding.to(dtype=prior_model.fc1.weight.dtype)\n",
    "\n",
    "        # Forward pass through diffusion prior model\n",
    "        noisy_latent = diffusion_prior_model(text_embedding)\n",
    "\n",
    "        # Compute loss with respect to actual CLIP image embedding\n",
    "        loss = criterion(noisy_latent, image_embedding)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the progress bar every 5000 batches\n",
    "        if (i + 1) % update_interval == 0:\n",
    "            pbar.update(update_interval)\n",
    "\n",
    "    # Close the progress bar at the end of the epoch\n",
    "    pbar.close()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(diffusion_prior_model.state_dict(), 'diffusion_prior_model.pth')\n",
    "\n",
    "# Inference: Load the trained model and get latent vectors\n",
    "def get_latent_vector(text_embedding):\n",
    "    diffusion_prior_model.eval()  # Set the model to evaluation mode\n",
    "    text_embedding = text_embedding.to(device)\n",
    "    text_embedding = text_embedding.to(dtype=prior_model.fc1.weight.dtype)\n",
    "    with torch.no_grad():\n",
    "        latent = diffusion_prior_model(text_embedding)\n",
    "    return latent\n",
    "\n",
    "# Example inference\n",
    "# Assuming you have a new text_embedding tensor for inference\n",
    "# new_text_embedding = torch.randn(1, input_dim).to(device)  # Example tensor\n",
    "latent_vector = get_latent_vector(new_text_embedding[len(new_text_embedding)-1])\n",
    "print(f'Latent vector : {latent_vector}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8114aa9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:53:02.496810Z",
     "iopub.status.busy": "2024-09-04T18:53:02.495950Z",
     "iopub.status.idle": "2024-09-04T18:53:02.502264Z",
     "shell.execute_reply": "2024-09-04T18:53:02.501334Z"
    },
    "papermill": {
     "duration": 3.482529,
     "end_time": "2024-09-04T18:53:02.504306",
     "exception": false,
     "start_time": "2024-09-04T18:52:59.021777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class DiffusionDecoder(nn.Module):\n",
    "#     def __init__(self, latent_dim, text_embedding_dim, image_embedding_dim, hidden_dim):\n",
    "#         super(DiffusionDecoder, self).__init__()\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.text_embedding_dim = text_embedding_dim\n",
    "#         self.image_embedding_dim = image_embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         # Combine text and latent embeddings\n",
    "#         self.fc1 = nn.Linear(latent_dim + text_embedding_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc3 = nn.Linear(hidden_dim, image_embedding_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, latent, text_embedding):\n",
    "#         # Concatenate latent vector and text embedding\n",
    "#         combined_input = torch.cat([latent, text_embedding], dim=-1)\n",
    "        \n",
    "#         # Pass through fully connected layers\n",
    "#         x = self.relu(self.fc1(combined_input))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         output = self.fc3(x)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# # Initialize Diffusion Decoder\n",
    "# latent_dim = 512  # Dimension of latent vector from prior model\n",
    "# text_embedding_dim = 512  # Dimension of text embedding from CLIP\n",
    "# image_embedding_dim = 512  # Dimension of CLIP image embedding\n",
    "# hidden_dim = 1024  # Hidden dimension size\n",
    "\n",
    "# diffusion_decoder = DiffusionDecoder(latent_dim, text_embedding_dim, image_embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# # Example usage\n",
    "# def generate_image_from_latent(latent, text_embedding):\n",
    "#     latent = latent.to(device)\n",
    "#     text_embedding = text_embedding.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         generated_image_embedding = diffusion_decoder(latent, text_embedding)\n",
    "#     return generated_image_embedding\n",
    "\n",
    "# # Example latent and text embeddings\n",
    "# example_latent = torch.randn(1, latent_dim).to(device)  # Example latent vector\n",
    "# example_text_embedding = torch.randn(1, text_embedding_dim).to(device)  # Example text embedding\n",
    "\n",
    "# # Generate image embedding\n",
    "# generated_image_embedding = generate_image_from_latent(example_latent, example_text_embedding)\n",
    "# print(f'Generated image embedding shape: {generated_image_embedding}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e85bdb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:53:09.419655Z",
     "iopub.status.busy": "2024-09-04T18:53:09.418783Z",
     "iopub.status.idle": "2024-09-04T18:53:09.425232Z",
     "shell.execute_reply": "2024-09-04T18:53:09.424357Z"
    },
    "papermill": {
     "duration": 3.434479,
     "end_time": "2024-09-04T18:53:09.427304",
     "exception": false,
     "start_time": "2024-09-04T18:53:05.992825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# # Define Diffusion Decoder Model\n",
    "# class DiffusionDecoder(nn.Module):\n",
    "#     def __init__(self, latent_dim, image_size):\n",
    "#         super(DiffusionDecoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(latent_dim, 512)\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.fc3 = nn.Linear(256, image_size * image_size * 3)\n",
    "#         self.relu = nn.ReLU()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#         x = x.view(-1, 3, image_size, image_size)\n",
    "#         return x\n",
    "\n",
    "# # Initialize Models\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# input_dim = 512\n",
    "# latent_dim = 512\n",
    "# image_size = 64\n",
    "\n",
    "\n",
    "# diffusion_decoder = DiffusionDecoder(latent_dim, image_size).to(device)\n",
    "\n",
    "# # Generate Latent Vector\n",
    "# def generate_latent(text_embedding):\n",
    "#     text_embedding = text_embedding.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         latent = prior_model(text_embedding)\n",
    "#         latent = latent.squeeze(1)\n",
    "#     return latent\n",
    "\n",
    "# # Generate Image from Latent Vector\n",
    "# def generate_image(latent_vector):\n",
    "#     latent_vector = latent_vector.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         generated_image = diffusion_decoder(latent_vector)\n",
    "#     return generated_image\n",
    "\n",
    "# # Convert Tensor to PIL Image\n",
    "# def tensor_to_pil(tensor):\n",
    "#     tensor = tensor.squeeze().cpu().detach().clamp(0, 1)\n",
    "#     array = tensor.permute(1, 2, 0).numpy()\n",
    "#     return Image.fromarray((array * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "# generated_image = generate_image(latent_vector)\n",
    "# generated_image_pil = tensor_to_pil(generated_image)\n",
    "# #generated_image_pil.show()\n",
    "# # Save the generated image to a file\n",
    "# image_path = '/tmp/generated_image.png'\n",
    "# generated_image_pil.save(image_path)\n",
    "\n",
    "# print(f\"Image saved to {image_path}\")\n",
    "\n",
    "\n",
    "# # Display the image in a Jupyter notebook\n",
    "# display(generated_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39edf74a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:53:16.315275Z",
     "iopub.status.busy": "2024-09-04T18:53:16.314916Z",
     "iopub.status.idle": "2024-09-04T18:53:20.721617Z",
     "shell.execute_reply": "2024-09-04T18:53:20.720613Z"
    },
    "papermill": {
     "duration": 7.866433,
     "end_time": "2024-09-04T18:53:20.723676",
     "exception": false,
     "start_time": "2024-09-04T18:53:12.857243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed image: torch.Size([1, 2, 32, 32])\n",
      "Image saved to /tmp/generated_image.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=LA size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = UNetBlock(in_channels, 64)\n",
    "        self.encoder2 = UNetBlock(64, 128)\n",
    "        self.encoder3 = UNetBlock(128, 256)\n",
    "        self.encoder4 = UNetBlock(256, 512)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Decoder\n",
    "        self.upsample1 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
    "        self.decoder3 = UNetBlock(512 + 256, 256)\n",
    "        \n",
    "        self.upsample2 = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = UNetBlock(256 + 128, 128)\n",
    "        \n",
    "        self.upsample3 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
    "        self.decoder1 = UNetBlock(128 + 64, 64)\n",
    "\n",
    "        # Final output\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(self.pool(x1))\n",
    "        x3 = self.encoder3(self.pool(x2))\n",
    "        x4 = self.encoder4(self.pool(x3))\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upsample1(x4)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        out = self.final_conv(x)\n",
    "        return out\n",
    "\n",
    "class DiffusionDecoder(nn.Module):\n",
    "    def __init__(self, timesteps, latent_dim, out_channels):\n",
    "        super(DiffusionDecoder, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.unet = UNet(in_channels=2, out_channels=out_channels)  # Adjusted in_channels to 2\n",
    "\n",
    "    def remove_noise(self, x, t):\n",
    "        # Implement the noise removal logic here\n",
    "        # Example: simple identity function, replace with actual noise removal\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            x = self.remove_noise(x, t)\n",
    "            x = self.unet(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "latent_dim = 512  # Dimension of latent representation\n",
    "image_dim = 2  # Assuming 3 channels for image output\n",
    "timesteps = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "diffusion_decoder_model = DiffusionDecoder(timesteps, latent_dim, image_dim).to(device)\n",
    "\n",
    "# Example inference\n",
    "latent_vector = torch.randn(1, 2, 512).to(device)  # Example tensor with latent shape [batch, 2, 512]\n",
    "\n",
    "# Reshaping and upsampling the latent vector\n",
    "height, width = 32, 32  # Desired spatial dimensions\n",
    "noisy_latent = latent_vector.view(1, 2, 1, 512)  # Reshape to [1, 2, 1, 512]\n",
    "noisy_latent = F.interpolate(noisy_latent, size=(height, width))  # Upsample to [1, 2, 32, 32]\n",
    "\n",
    "# Pass the upsampled latent vector through the model\n",
    "reconstructed_image = diffusion_decoder_model(noisy_latent)\n",
    "print(f'Reconstructed image: {reconstructed_image.shape}')\n",
    "\n",
    "# Convert Tensor to PIL Image\n",
    "def tensor_to_pil(tensor):\n",
    "    tensor = tensor.squeeze().cpu().detach().clamp(0, 1)\n",
    "    array = tensor.permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((array * 255).astype(np.uint8))\n",
    "\n",
    "generated_image_pil = tensor_to_pil(reconstructed_image)\n",
    "image_path = '/tmp/generated_image.png'\n",
    "generated_image_pil.save(image_path)\n",
    "print(f\"Image saved to {image_path}\")\n",
    "\n",
    "# Display the image in a Jupyter notebook (if applicable)\n",
    "display(generated_image_pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29af432d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-04T18:53:27.749250Z",
     "iopub.status.busy": "2024-09-04T18:53:27.748873Z",
     "iopub.status.idle": "2024-09-04T18:53:27.755227Z",
     "shell.execute_reply": "2024-09-04T18:53:27.754204Z"
    },
    "papermill": {
     "duration": 3.582084,
     "end_time": "2024-09-04T18:53:27.757225",
     "exception": false,
     "start_time": "2024-09-04T18:53:24.175141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 32, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_latent.shape"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3007402,
     "sourceId": 5173923,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3445072,
     "sourceId": 6019472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5557058,
     "sourceId": 9192360,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5565663,
     "sourceId": 9205070,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5568917,
     "sourceId": 9209952,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5569939,
     "sourceId": 9211475,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6797.249112,
   "end_time": "2024-09-04T18:53:34.255013",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-04T17:00:17.005901",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
