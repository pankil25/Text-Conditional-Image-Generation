
import os
import torch
from pycocotools.coco import COCO
from PIL import Image
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm

# Path to images and annotations
image_dir = "coco/train2017/"
ann_file = "coco/annotations/captions_train2017.json"

# Initialize COCO API
coco = COCO(ann_file)

# Get the first 100 image ids
img_ids = list(coco.imgs.keys())[:100]

# Define a custom dataset class
class CocoDataset(Dataset):
    def __init__(self, img_ids, image_dir, coco, transform=None):
        self.img_ids = img_ids
        self.image_dir = image_dir
        self.coco = coco
        self.transform = transform

    def __len__(self):
        return len(self.img_ids)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)
        caption = anns[0]['caption']

        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.image_dir, img_info['file_name'])
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, caption

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# Create the dataset and dataloader
dataset = CocoDataset(img_ids=img_ids, image_dir=image_dir, coco=coco, transform=transform)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# Load the CLIP model and processor
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Training loop
model.train()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

for epoch in range(3):  # Train for 3 epochs
    total_loss = 0
    for images, captions in tqdm(dataloader):
        # Process images and captions
        inputs = processor(text=captions, images=images, return_tensors="pt", padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        optimizer.zero_grad()
        outputs = model(**inputs)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}")

# Evaluation
model.eval()
with torch.no_grad():
    for images, captions in dataloader:
        inputs = processor(text=captions, images=images, return_tensors="pt", padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image  # Image-to-text similarity score
        logits_per_text = outputs.logits_per_text    # Text-to-image similarity score

        # Print the logits for the first example in the batch
        print(logits_per_image[0], logits_per_text[0])

